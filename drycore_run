#!/bin/bash
################################################################################
# README.md file has instructions on namelist/diagnostic table parameters
# This script runs the GFDL dry core model in blocks of N days, and concurrently
# processes data with "process" script while next block is run.
# Concurrent processing can hugely reduce the amount of time needed to get derived
# quantities like heat flux and stuff.
################################################################################
# Initial stuff 
################################################################################
ulimit -s unlimited # set max open files
cwd=$(pwd)
cores=8      # number of cores for parallel
mode=0       # default is to run the control
new=false    # do not write to existing experiment directory?
resume=false # do not overwrite existing run block?
all=false  # keep xyz data for all days?
none=false # throw out all data for all days (i.e. just keep restarts)?

# Executables
# export MPI_SHEPHERD=true # see: https://www2.cisl.ucar.edu/sites/default/files/intro_to_hpc.pdf
# when mpiexec_mpt used, MPI_SHEPHERD required, but this failed for me
# Make sure module load impi has been declared by parent
run=mpirun # for running in parallel
fms=$cwd/fms.x # location of executables
process=$cwd/process # *process the parallel-output nc files directly before merging*
[ ! -x $fms ]     && echo "Error: The executable $fms is missing."                 && exit 1
[ ! -x $process ] && echo "Error: The bash script for processing data is missing." && exit 1
! which $run &>/dev/null && echo "Error: $run not found in \$PATH." && exit 1

# Parse input flags
# Read the comments to see explanations for each option
while [ $# -gt 0 ]; do # echo "Flag: $1"
  case "$1" in
    -n|--new)         new=true ;;       # use this to *exit* if experiment folder already exists
    -r|--resume)      resume=true ;;    # use this to *autodetect* last day of existing run, continue from there
    -ts=*|--tstart=*) tstart=${1#*=} ;; # use this to continue from *particular* day of existing run
    -te=*|--tend=*)   tend=${1#*=} ;;   # ending day
    -c=*|--cores=*)   cores=${1#*=} ;;
    -m=*|--mode=*)    mode=${1#*=} ;;   # 0 is control, 1 is spindown, 2...
    -p=*|--process=*)  pflags_global="${1#*=}" ;; # flags for processing script
    -rd=*|--restart=*) rdir_override=${1#*=} ;;   # manual override restart directory
    -ds=*|--days-spindown=*)  t_spindown=($(echo ${1#*=} | tr ',' ' ')) ;;
    -dxyz=*|--days-xyzdata=*) t_xyzdata=($(echo ${1#*=}  | tr ',' ' ')) ;;
    -dn=*|--days-nodata=*)    t_nodata=($(echo ${1#*=}   | tr ',' ' ')) ;;
    -*) echo "Error: Unknown flag \"$1\"." && exit 1 ;;
    *)  [ -n "$expdir" ] && echo "Error: More than one experiment directory specified." && exit 1
      expdir="$1" ;;
  esac; shift
done
[ -z $expdir ]               && echo "Error: You must declare the experiment directory." && exit 1
[[ -z $tstart || -z $tend ]] && echo "Error: Start and end times must be declared."      && exit 1
[ ${#t_nodata[@]} -gt 0 ]    && echo "Removing all data for days: ${t_nodata[@]}."
[ ${#t_xyzdata[@]} -gt 0 ]   && echo "Preserving full resolution for days: ${t_xyzdata[@]}."
[[ $mode -ne 0 && ${#t_spindown[@]} -gt 0 ]] && echo "Starting spindown for days: ${t_spindown[@]}"

# Optionally exit from script if directory already exists
$new && [ -d $expdir ] && echo "Error: Working directory already exists. Continuing..." && exit 1
if [ ! -d $expdir ]; then
  mkdir $expdir # make directory
  [ $? -ne 0 ] && echo "Error: Could not create experiment directory \"$expdir\"." && exit 1
fi

# Usage is array_in value ${array[@]}, behaves like python 'value in array'
array_in() {
  [[ " ${@:2} " =~ " ${1} " ]]
  return $?
}
# Get namelist value for the *global*, experiment-series namelist
nml_parse() {
  cat $expdir/input.nml | sed 's/!.*//g' | grep "$1" | cut -d= -f2 | tr -d "\t ,'\""
}
# Replace namelist value for a *local* spindown experiment
nml_replace() { # first argument is param name, second argument is value
  while [ $# -gt 0 ]; do
    ! grep '^[ \t]*\b'${1}'\b' ./input.nml &>/dev/null && \
      echo "Error: Param \"${1}\" not found in namelist." && exit 1
    space='\([ \t]*\)' # space atom; more readable to set it as a variable
    echo "Replacing param \"${1}\" with \"${2}\"."
    sed -i 's/^'"${space}${1}${space}"'='"${space}"'.*$/\1'${1}'\2=\3'${2}',/g' ./input.nml
    shift 2
    [ $? -ne 0 ] && echo "Error: Must pass even number of params to nml_replace function."
  done
}
days=$(nml_parse days) # number of days in each block
# Process exit status
exit_check() {
  estatus=$1
  if [ $estatus != 0 ]; then
    echo "Error: Exit status $estatus from post-processing previous block..."
    case $estatus in
      1) echo "NetCDF files missing. Check log.model." ;;
      2) echo "Something failed during pressure level interpolation. Check log.interp." ;;
      3) echo "Something failed while getting basic terms. Check log.means." ;;
      4) echo "Something failed while getting param terms. Check log.params." ;;
      *) echo "Check log.process; other/miscellaneous failure." ;;
    esac
    exit 1
  fi
}

################################################################################
# Function for running the next model step from a previous step
# applying post-processing to a previous model step in the background, and organizing
# all the NetCDF files
################################################################################
driver() {
  # Run model in parallel
  # Previously we set processor affinities
  local rdir=$1 # the restart direcotry
  [ $# -ne 1 ] && echo "Error: driver() function takes exactly 1 argument." && exit 1
  t1=$(date +%s)
  echo "Running model..."
  $run -np $cores ./fms.x &>log.model # need ./fms.x, not fms
  egrep 'EXIT CODE: [1-9]|FATAL from PE|WARNING from PE' log.model &>/dev/null && \
    echo "Error: Bad exit code from model run step." && exit 1
  tmodel=$(cat log.model | grep "Total runtime*" | xargs | cut -d " " -f 5)
  echo "Model time: ${tmodel%.*}s." # is just max of the two
  # taskset -p $(taskset -p $$ | cut -d ':' -f 2) $! # match processor affinities
  # taskset -cp 0-19 $! 1>/dev/null # $! == pid of last job run in background

  # Check that model ran successfully
  # FMS prints error messages to standard output but doesn't actually set
  # the exit code/mpirun doesn't pass that exit code, which is fucking dumb.
  # We parse the logfile instead
  # egrep 'EXIT CODE: [1-9]|FATAL from PE' log.model &>/dev/null && \
  #   echo "Error: Bad exit code from model run step." && exit 1
  # Wait for previous processing step, if set
  # If unset, means we are just now starting the model run
  # if [ -n "$pp" ]; then # post-processing is active
  #   echo "Wating for process: $pp"
  #   wait $pp # wait command will always exit with exit status of supplied process ID
  #   exit_check $?
  # fi
  # Echo timing information from previous run
  # Processing was run in parallel, remember
  # tmodel=$(cat log.model | grep "Total runtime*" | xargs | cut -d " " -f 5)
  # echo "Model time: ${tmodel%.*}s." # is just max of the two
  # if [ -r $rdir/log.process ]; then
  #   tprocess=$(tail -1 $rdir/log.process | sed 's/[^0-9]*//g' )
  #   echo "Model time: ${tmodel%.*}s, Previous process time: ${tprocess%.*}s."
  # else
  #   echo "Model time: ${tmodel%.*}s." # is just max of the two
  # fi

  # Remove some files
  # [ -r INPUT/topography.data.nc ] && rm INPUT/topography.data.nc # remove topography
  [ -d INPUT ] && rm -r INPUT # remove everything
  [ -r logfile.0000.out ] && mv logfile.0000.out log.init # contains init info
  rm fms.x # remove executable, because takes up space

  # Process new data, and remove old data
  # Previously we set processor affinities, but didn't help much
  echo "Calling processing script with flags: $(echo $pflags_day $pflags_global | xargs)"
  $process $pflags_day $pflags_global &>log.process
  exit_check $?
  tprocess=$(tail -1 log.process | sed 's/[^0-9]*//g')
  echo "Process time: ${tprocess%.*}s."
  # taskset -p $(taskset -p $$ | cut -d ':' -f 2) $!
  # taskset -cp 20-23 $! 1>/dev/null # send to particular CPUs
}

################################################################################
# Set up input files for model executable to read
# Takes two arguments: 1) the working directory, and 2) the iteration mode
################################################################################
dir_setup() {
  # Set up working directory, and move there
  wdir="$1" # where to move files
  [ $# -ne 1 ] && echo "Error: dir_setup() functions requires exactly 1 argument." && exit 1
  if ! [ -d $wdir ]; then
    # Resume run here
    resume=false
    echo "Setting up working directory ${wdir##*/}..."
  elif $resume; then
    # if compgen -G "$wdir/*.nc" &>/dev/null; then
    #   echo "Working directory ${wdir##*/} contains failed processing step. Deleting..."
    # elif [ -z "$(\ls $wdir/RESTART)" ]; then # need to check *contents*; don't care about empty one
    #   echo "Working directory ${wdir##*/} contains unfinished integration. Deleting..."
    # else
    #     echo "Working directory ${wdir##*/} contains completed integration. Cancelling..."
    #   return 1 # breaks out of if statement
    # fi
    if compgen -G "$wdir/../netcdf/*${wdir##*/}.nc" &>/dev/null; then
      echo "NetCDF files already exist for block ${wdir##*/}. Cancelling..."
      return 1 # breaks out of if statement
    else
      echo "No NetCDF files found for block ${wdir##*/}. Resuming..."
    fi
    # Otherwise, resume run and delete 'unfinished' contents
    resume=false
    rm -r $wdir
  else
    echo "Working directory ${wdir##*/} already exists. Deleting..."
    rm -r $wdir
  fi

  # Make directory and move stuff over
  mkdir $wdir
  [ $? -ne 0 ] && echo "Error: Failed to create working directory \"$wdir\"." && return 1
  cd $wdir
  cp $fms ./fms.x # move executable inside (declared at top of file)
  mkdir RESTART # model spits out stuff here, can be accepted as input to new iteration
  mkdir INPUT   # model reads from this
  touch field_table # just put empty file, if want no tracers

  # Copy existing namelist file over
  # For shutdown experiments, can edit on the fly to turn off
  # radiation, et cetera, but this is starting point
  [ ! -r "$expdir/input.nml" ]  && echo "Error: input.nml file not found."  && exit 1
  [ ! -r "$expdir/diag_table" ] && echo "Error: diag_table file not found." && exit 1
  cp $expdir/input.nml  ./
  cp $expdir/diag_table ./
  topo=$(nml_parse "topography_option") # use helper function
  if [ "$topo" == "input" ]; then
    [ ! -r $expdir/topography.data.nc ] && echo "Error: Topography file not available." && exit 1
    cp $expdir/topography.data.nc ./
  fi

  # Change namelist properties
  if [ $mode -gt 0 ]; then case $mode in   # determine spindown type
    1) nml_replace k_trop 0 k_bl 0 ;;          # turn off all thermal damping
    2) nml_replace k_trop 0 ;;                 # turn off damping except in boundary layer
    3) nml_replace k_trop 0 k_bl 0 k_fric 0 ;; # turn off all damping
    4) nml_replace k_fric 0 ;;                 # turn off friction
    *) echo "Error: Unknown experiment identifier \"$mode\"."; exit 1 ;;
  esac; fi
  # Parent script will test return code; ensure zero here
  return 0
}

################################################################################
# Function for restarting model; put correct files in correct place so 
# fms can read them and continue iteration from a previous state.
################################################################################
# Take one argument: directory where restart files exist
copy_restart() {
  # Copy over relevant restart files for exp type; if missing, raise error
  # resfiles="atmos_model.res atmos_tracers.res.nc fv_rst.res.nc fv_srf_wnd.res.nc" # fv type
  # resfiles="atmos_model.res atmos_tracers.res.nc bgrid_prog_var.res.nc" # brid type
  local rdir=$1 # the restart direcotry
  [ $# -ne 1 ] && echo "Error: copy_restart() function takes exactly 1 argument." && exit 1
  if [ ! -d "$rdir" ] || [ ! -d "$rdir/RESTART" ] || [ -z "$(ls $rdir/RESTART/* 2>/dev/null)" ]; then
    echo "Error: Restart directory $rdir/RESTART does not exist, or is empty." && exit 1
  fi
  resfiles="atmos_model.res atmosphere.res.nc spectral_dynamics.res.nc"
  echo "Moving restart files from ${rdir##*/}/RESTART to ${PWD##*/}/INPUT..."
  for file in $resfiles; do
    [ ! -r $rdir/RESTART/$file ] && echo "Error: Missing restart file ${rdir##*/}/RESTART/${file}." && exit 1
    cp $rdir/RESTART/$file INPUT/$file
  done
}

##############################################################################
# CONTROL RUN
# Run the model in blocks of $days days for control, then optionally choose
# starting points from control for spin-down ensemble experiments
##############################################################################
estatus=0
case $mode in
  0) # CHECK THAT TIMING VARIABLES ARE DECLARED
  # PREPARE FOR THE LOOP
  echo "Running control experiment from day $tstart to day $tend on $cores cores, restart every $days days."
  coldstart=true # assume cold start by default
  t0=$(date +%s) # start time
  pday=0 # only time when we do minus days
  cday=0
  nday=$days
  while [ $nday -le $tend ]; do
    # MESSAGE AND RESET TIMER/FLAGS
    # SKIP THIS TIME (OPTIONALLY)
    if [ $cday -lt $tstart ]; then
      pday=$cday # previous day
      cday=$((pday + days))
      nday=$((cday + days))
      continue
    fi
    echo "Running from day $cday to day $nday."
    time=$(date +%s)
    # RUN THE MODEL and COMBINE OUTPUT
    # OPTIONALLY USE THE END OF OTHER CONTROL RUNS TO REDUCE SPINUP TIME
    rdir=$expdir/d$(printf "%04d" $pday)-d$(printf "%04d" $cday)
    cdir=$expdir/d$(printf "%04d" $cday)-d$(printf "%04d" $nday)
    if [ ! -z $rdir_override ]; then coldstart=false
      if [[ ! $rdir_override =~ d.*-d.* ]]; then
        echo "Override: Using final day from \"$rdir_override\" for restart files."
        rdir_override=($rdir_override/d*-d*); rdir="${rdir_override[-1]}" # pick last one; should be sorted
      else
        echo "Override: Using restart files from \"$rdir_override\"."
        rdir="$rdir_override" # use specific day sequence
      fi; unset rdir_override # only ever use this on first 'day' of a new experiment; for subsequent days, continue from earlier day block
      [ ! -d $rdir ] && echo "Error: Override restart directory \"$rdir\" does not exist." && exit 1
    fi
    unset pflags_day
    array_in $cday ${t_nodata[@]} && pflags_day+=" -q" && echo "Will delete output netcdf files."
    array_in $cday ${t_xyzdata[@]} && pflags_day+=" -k" && echo "Will keep XYZ netcdf files."
    dir_setup $cdir # sets up working directory, cd into it
    if [ $? -eq 0 ]; then # setup returns 1 if directory is present and 'resume' option is set
      if ! $coldstart || [ $cday -gt 0 ]; then
        copy_restart $rdir # put files into RESTART directory
      else
        echo "Cold start."
      fi
      driver $rdir # run model
    fi
    # EXIT FOR INITIAL CONDITION EXPERIMENT
    [ $days -eq 0 ] && break
    # STEP THINGS FORWARD, FOR NEXT ITERATION
    pday=$cday
    cday=$((pday + days))
    nday=$((cday + days))
  done
  # LAST FILE
  echo "Processing last file..."
  wait $pp
  exit_check $?
  unset pp
  tprocess=$(tail -1 log.process | sed 's/[^0-9]*//g')
  echo "Final process time: ${tprocess%.*}s."
  # MESSAGE
  echo "The control run completed successfully in $(($(date +%s) - t0)) seconds!"
  echo "Timestamp: $(date)."

##############################################################################
# SPINDOWN RUNS
# User must specify which namelist params are getting abruptly changed.
##############################################################################
  ;; *) # CHECK THAT TIMING VARIABLES ARE DECLARED
  [ -z $t_spindown ] && echo "Error: Must declare starting times for spindown experiments." && exit 1
  # PREPARE FOR THE LOOP
  echo "Running spindown experiment $mode from days ${t_spindown[@]} for $tend days, restart every $days days."
  t0=$(date +%s)
  # ITERATE THROUGH STARTING DAYS
  for eday in "${t_spindown[@]}"; do
    cday=0 # current day relative to start of equilibrium
    nday=$days # next day, relative to start
    prefix=$expdir/d$(printf "%04d" $eday) # for successive spindown runs
    fstart=$expdir/d$(printf "%04d" $((eday - days)))-d$(printf "%04d" $eday) # for restart files from control
    ts=$(date +%s) # record time
    echo "Starting radiation-off spindown run from day $eday for $tend days."
    while [ $nday -le $tend ]; do
      # SKIP THIS TIME (OPTIONALLY)
      if [ $cday -lt $tstart ]; then
        echo "Skipping day $cday."
        pday=$cday # previous day
        cday=$((pday + days))
        nday=$((cday + days))
        continue
      fi
      # GET DIRECTORIES
      cdir=$prefix-spindown$mode-d$(printf "%04d" $cday)-d$(printf "%04d" $nday)
      [ $cday -eq 0 ] && rdir=$fstart || \
        rdir=$prefix-spindown$mode-d$(printf "%04d" $pday)-d$(printf "%04d" $cday)
      # RUN THE MODEL and COMBINE OUTPUT
      unset pflags_day
      array_in $cday ${t_nodata[@]} && pflags_day+=" -q"
      array_in $cday ${t_xyznodata[@]} && pflags_day+=" -k"
      dir_setup $cdir # sets up working directory, cd into it
      if [ $? == 0 ]; then # returns 1 if we were requested not to overwrite old directories
        copy_restart $rdir # add restart files
        driver $rdir # run model
      fi
      # STEP THINGS FORWARD, FOR NEXT ITERATION
      pday=$cday # previous day
      cday=$((pday + days))
      nday=$((cday + days))
    done
    echo "Spindown from $eday completed successfully in $(($(date +%s) - ts)) seconds!"
    echo "Timestamp: $(date)."
  done
  echo "Processing last file..."
  tprocess=$(tail -1 log.process | sed 's/[^0-9]*//g')
  wait $pp; pp=
  echo "The spindown runs completed successfuly in $(($(date +%s) - t0)) seconds!"
  echo "Timestamp: $(date)."

################################################################################
# Other experiments types can go below
################################################################################
  ;; *) echo "Error: Unknown experiment type \"$mode\"." && exit 1 ;;
esac

