#!/bin/bash
################################################################################
# This script post-processes the raw model output files generated from the
# GFDL dry core model. Needs to be supplied with a source and destination directory.
# ***Warning*** destination directory should really be on a backed up disk.
# This script was recently modified, but should be fine now
################################################################################
# Verify environmental variables are set
# flags='-O'
flags='-s -O' # overwrite, and only issue warnings
shopt -s nullglob # will return empty if no match
debug=false # disable sending stuff to background?

# Optional environmental variables, with defaults
# The flag names should be descriptive enough, but feel free to read comments
climo_start=0
climo_end=100000
spectral_climate=false
control_climate=false
control_regional_ts=false
control_climate_isen=false
control_regional_ts_isen=false
control_energy_rho=false
control_energy_ts=false
# Objective analysis stuff, energy time series
control_eof1d=false
control_eof2d=false
# Spindown options
spin_start=0
spin_end=100000
spindown_timescale=false
spindown_ensemble_ts=false
spindown_regional_ts=false
# Parse user input
mode=2 # spindown mode
while [ $# -gt 0 ]; do
  case $1 in
    -d|--debug)                 debug=true                ;;
    -m=*|--mode=*)              mode=${1##*=}             ;;
    -c1=*|--climo-start=*)      climo_start=${1##*=}      ;;
    -c2=*|--climo-end=*)        climo_end=${1##*=}        ;;
    -s1=*|--spin-start=*)       spin_start=${1##*=}       ;;
    -s2=*|--spin-end=*)         spin_end=${1##*=}         ;;
    -sc|--spectral-climo)       spectral_climate=true ;;
    -cc|--control-climo)        control_climate=true      ;;
    -crt|--control-region-ts)   control_regional_ts=true  ;;
    -cci|--control-climo-isen)        control_climate_isen=true      ;;
    -crti|--control-region-ts-isen)   control_regional_ts_isen=true  ;;
    -cer|--control-energy-rho)  control_energy_rho=true   ;;
    -cet|--control-energy-ts)   control_energy_ts=true    ;;
    -e1d|--control-eof1d)       control_eof1d=true        ;;
    -e2d|--control-eof2d)       control_eof2d=true        ;;
    -se|--spindown-ensemble-ts) spindown_ensemble_ts=true ;;
    -sr|--spindown-region-ts)   spindown_regional_ts=true ;;
    -st|--spindown-timescale)   spindown_timescale=true   ;;
    -*) echo "Error: Unknown flag \"${1}\"." && exit 1 ;;
    *) # 3 required arguments, set in this order
      if [ -z $filename ]; then
        filename="$1"
      elif [ -z $input ]; then
        input="$1"
      elif [ -z $output ]; then
        output="$1"
      else echo "Error: Too many arguments." && exit 1
      fi ;;
  esac
  shift
done
if [ -z "$filename" ] || [ -z "$input" ] || [ -z "$output" ]; then
  echo "Error: Need filename prefix (e.g. 2xdaily_inst), input folder, and output folder, in that order."
  exit 1
fi

# File management/names
# Prefixes for output files
plog=log_ # log prefix; easy to change by making it a variable
pfull=${filename}_full
psummary=${filename}_summary # original prefix file names
pspectral=${filename}_spectral
pfull_isen=${filename}_full_isen
psummary_isen=${filename}_summary_isen
# Destinations for final files
fspectral=${filename}_spectral.nc
fclimate=${filename}_climate.nc
fclimate_isen=${filename}_climate_isen.nc
fregion=${filename}_timeseries.nc
fregion_isen=${filename}_timeseries_isen.nc
fenergy=${filename}_energy.nc
fautocorr=${filename}_autocorr.nc
fspinclimate=${filename}_spindown${mode}xs.nc # cross-section
fspindownpoles=${filename}_spindown${mode}poles.nc # spindown rate at the poles
ftimescale=${filename}_timescale${mode}.nc # the timescale stuff

# Directory management; move to save directory
cwd=$(pwd) # directory where scripts stored (you must run this script from current directory!)
! [ -d $input ] && echo "Error: Cannot find input directory \"$input\"." && exit 1
! [ -d $output ] && mkdir $output
cd $output # move here

# Energy terms
ts_params=ke,km,pe,pm,ehf,emf
energy_params=ckekm,cpeke,cpmkm,cpmpe,dke,dkm,gpe,gpm,ke,km,pe,pm

# NetCDF utils copied from bashrc
nclist() {
  command ncdump -h "$1" | sed -n '/variables:/,$p' | sed '/^$/q' | grep -v '[:=]' \
    | cut -d '(' -f 1 | sed 's/.* //g' | xargs | tr ' ' '\n' | grep -v '[{}]' | xargs
}
ncdims() { # get list of dimensions
  command ncdump -h "$1" | sed -n '/dimensions:/,$p' | sed '/variables:/q' \
    | cut -d '=' -f 1 -s | xargs | tr ' ' '\n' | grep -v '[{}]' | xargs
}
ncvars() { # only get text between variables: and linebreak before global attributes
  local list=($(nclist "$1"))
  local dmnlist=($(ncdims "$1"))
  local varlist=() # add variables here
  for item in "${list[@]}"; do
    if [[ ! " ${dmnlist[@]} " =~ " $item " ]]; then
      varlist+=("$item")
    fi
  done
  echo "${varlist[@]}" | tr -s ' ' '\n' | grep -v '[{}]' | xargs # print results
}
ncin() {
  nclist $1 | grep $2 &>/dev/null
}

# Helper functions
# Wait for processes from array of process IDs, and check their exit statuses
call_parallel() {
  if $debug; then # don't send to background
    t=$(date +%s)
    $@ # just naked call to command
    echo "Elapsed time: $(($(date +%s) - t))."
  else
    $@ &>$log & # send to background, save logfile
  fi
}
check_exits() {
  local processes=($@)
  for process in ${processes[@]}; do
    wait $process
    exitnums+=($?) # if process already done, wait just mimicks its exit status
  done
  local i=0 # iterate
  for exitnum in ${exitnums[@]}; do
    if [ $exitnum -ne 0 ]; then
      echo "Error: One of the background processes failed: \"$(ps -p ${processes[$i]} -o comm=)\"."
    fi
    i=$(($i+1)) # forward
  done
  unset process exitnum
}
region_selbox() {
  region="$1"
  case $region in
    [nN][hH])     selregion="-sellonlatbox,-180,180,20,70"   ;; # north-hemisphere selection
    [sS][hH])     selregion="-sellonlatbox,-180,180,-70,-20" ;; # south-hemisphere selection
    pole[nN][hH]) selregion="-sellonlatbox,-180,180,60,90"   ;;
    pole[sS][hH]) selregion="-sellonlatbox,-180,180,-90,-60" ;;
    globe)        selregion=""                               ;;
    *) echo "Error: Invalid region ${region}." && return 1 ;;
  esac
  echo $selregion
}
print_days() {
  # Print list of days given array
  unset days
  for file in $@; do
    daystring=${file##*/}
    daystring=${daystring%.nc}
    daystring=${daystring#*.}
    day1=${daystring%%-*}
    day1=${day1#d}
    day2=${daystring##*-}
    day2=${day2#d}
    days+=($(printf "%.0f" $day1))
  done
  echo ${days[@]}
}

# Climate files within day range (e.g. to ignore spinup)
# Used for climate means and climate variability stuff
# climate_files() {
get_files() {
  # Glob
  local prefix file files ffiles start end
  local daystring day1 day2
  prefix=$1
  start=$climo_start # the *default* starting/ending points
  end=$climo_end
  [ -n "$2" ] && start=$2
  [ -n "$3" ] && end=$3
  files=($input/${prefix}.d????-d????.nc)
  [ ${#files[@]} -eq 0 ] && echo "Error: No $prefix files found in ${input}." 1>&2 && exit 1
  for file in ${files[@]}; do
    daystring=${file##*/}
    daystring=${daystring%.nc}
    daystring=${daystring#*.}
    day1=${daystring%%-*}
    day1=${day1#d}
    day2=${daystring##*-}
    day2=${day2#d}
    if [ $day1 -ge $start ] && [ $day2 -le $end ]; then
      ffiles+=($file)
    fi
  done
  [ ${#ffiles[@]} -eq 0 ] && echo "Error: No $prefix files found between day $start and ${end}." && exit 1
  echo "Between $start and $end, found days $(print_days ${ffiles[@]})." 1>&2
  echo ${ffiles[@]} # intended for user to capture output
}

# Spindown files grouped by initialization (branch) day
# Used for determining if spindowns are 'unique'
# TODO: Better print message
spindown_files_by_init() {
  echo "Getting list of spindown files within days $spin_start to $spin_end, grouped by control run initiation day."
  startdays=()
  for file in $input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-d[0-9][0-9][0-9][0-9]-d[0-9][0-9][0-9][0-9].nc; do
    startday=${file#$input/${psummary}.}
    startday=${startday%%-*} # the spindown day
    startdays+=($startday)
  done
  startdays=($(echo "${startdays[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')) # unique days
  spindowntsfiles=()
  for startday in ${startdays[@]}; do
    spindowngroup="" # for particular days
    for file in $input/${psummary}.${startday}-spindown$mode-d[0-9][0-9][0-9][0-9]-d[0-9][0-9][0-9][0-9].nc; do
      daystring=${file#$input/${psummary}.${startday}-spindown$mode-} # trim leading pattern
      daystring=${daystring%.nc}
      day1=${daystring%%-*} day1=${day1#d}
      day2=${daystring##*-} day2=${day2#d}
      [ $day1 -ge $spin_start ] && [ $day2 -le $spin_end ] && spindowngroup+=" $file"
    done
    spindowntsfiles+=("$spindowngroup") # the group of files is preserved as single array element separated by strings
  done
  [ ${#spindowntsfiles[@]} -eq 0 ] && echo "Error: No $psummary files found between days ${spin_start} and ${spin_end}." && exit 1
}
# echo "${spindowntsfiles[@]##*/}"
# for spindowngroup in "${spindowntsfiles[@]}"; do # testing
#   echo NEW GROUP; newgroup=(${spindowngroup}); echo ${newgroup[@]##*/}
# done

# Spindown files grouped by daystring (days after control run)
# Used for dynamical timescale stuff
# TODO: Better print message
spindown_files_by_day() {
  echo "Getting list of spindown files within days $spin_start to $spin_end, grouped by run day."
  daystrings=()
  for file in $input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-d[0-9][0-9][0-9][0-9]-d[0-9][0-9][0-9][0-9].nc; do
    daystring=${file#$input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-} # trim leading pattern
    daystring=${daystring%.nc}
    day1=${daystring%%-*} day1=${day1#d}
    day2=${daystring##*-} day2=${day2#d}
    [ $day1 -ge $spin_start ] && [ $day2 -le $spin_end ] && daystrings+=($daystring)
  done
  daystrings=($(echo "${daystrings[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')) # unique days
  spindowndayfiles=()
  for daystring in ${daystrings[@]}; do
    spindowngroup="" # for particular days
    for file in $input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-${daystring}.nc; do
      spindowngroup+=" $file"
    done
    spindowndayfiles+=("$spindowngroup") # the group of files is preserved as single array element separated by strings
  done
  [ ${#spindowndayfiles[@]} -eq 0 ] && echo "Error: No $psummary files found between days ${spin_start} and ${spin_end}." && exit 1
}
# echo "${spindowndayfiles[@]##*/}"
# for spindowngroup in "${spindowndayfiles[@]}"; do # testing
#   echo NEW GROUP; newgroup=(${spindowngroup}); echo ${newgroup[@]##*/${psummary}.}
# done

################################################################################
# Basic stuff, climate, averages
################################################################################
# Control climate data, and reference to other control climates
# Notes:
# * Cannot chain the merge command; must be highest level no matter what. So
#   prepend timmean commands to files being merged.
# * For some reason doing -merge on chained/interpolated data results in duplicate
#   pressure level coordinates; interpolating them individually avoids this, and 
#   explicitly re-declaring the z-axis description also avoids this.
control_climate() {
  prefix=$1
  out=$2
  climofiles=($(get_files $prefix))
  echo "Getting time-means."
  commands=("${climofiles[@]/#/ -timmean }")
  cdo $flags -ensmean ${commands[@]} $out
  # TODO: CDO library issue, maybe has conflict with Dask
#   python3 <<EOF
# import xarray as xr
# ds = xr.open_mfdataset([$(echo ${climofiles[@]} | sed 's/[^ ][^ ]*/"&", /g')], decode_times=False, concat_dim='record', decode_cf=False, chunks={'time':40}) # 40 out of 200
# ds = ds.mean(dim='time', keep_attrs=True) # by default attrs thrown out for all computations, which makes sense
# ds = ds.mean(dim='record', keep_attrs=True) # delay until times are averaged
# ds.to_netcdf("$out")
# EOF
}
# Average spectral runs
# More complicated because no record dimension
# Use undocumented feature referenced here: https://sourceforge.net/p/nco/discussion/9830/thread/cee4e1ad/
# A 'negative' length indicates unlimited
# Tried using the documented feature NC_UNLIMITED: http://nco.sourceforge.net/nco.html#Dimensions
# but did not work. NOTE: No longer do this so forget it.
spectral_climate() {
  prefix=$pspectral
  out=$fspectral
  climofiles=($(get_files $prefix))
  echo "Getting average spectra."
  # Merge with ncecat
  # NOTE: Way slower than CDO, but CDO does not support these file types. Bummer.
  # ncea ${climofiles[@]} $out
  # Merge with xarray and Dask
  # The fancy sed transforms 'a b c d' to '"a", "b", "c", "d", '
  python3 <<EOF
import xarray as xr
ds = xr.open_mfdataset([$(echo ${climofiles[@]} | sed 's/[^ ][^ ]*/"&", /g')], concat_dim='record', decode_cf=False)
ds = ds.mean(dim='record', keep_attrs=True) # by default attrs thrown out for all computations, which makes sense
ds.to_netcdf("$out")
EOF
}

# This function preserves levels, and takes means from 20deg to 70deg
# Used for the spectral analysis later
# TODO: ***FOUND A BUG***: For whatever reason, putting the region-selection
# before selname causes error, but after selname works fine.
control_regional_ts() {
  # Just pick a couple simple variables
  # Can be re-run no problem
  prefix=$1
  out=$2
  allfiles=($(get_files $prefix 0 100000)) # full record
  echo "Getting time series of hemisphere means."
  for region in SH NH; do
    selregion=$(region_selbox $region)
    commands=("${allfiles[@]/#/ -fldmean -selname,$ts_params $selregion }")
    # cdo -v -O ${commands[0]} test.nc; echo ${commands[0]}; echo Region $region; exit 1
    cdo $flags -mergetime ${commands[@]} ${out%.nc}${region}.nc # merge the means
  done
}

# Time series of global energy terms
# For this we use "all files", i.e. include spinup before climo_start and any
# days after climo_end; use these files to monitor spinup progress
control_energy_ts() {
  prefix=$psummary # has to be this
  allfiles=($(get_files $prefix 0 100000)) # full record
  echo "Getting time series of energy budget."
  commands=("${allfiles[@]/#/ -mulc,101325 -vertmean -fldmean -selname,$energy_params }")
  cdo $flags -mergetime ${commands[@]} $fenergy # merge the means
}

# Lag-1 autocorrelation for North et al. estimates
control_energy_rho() {
  prefix=$psummary
  climofiles=($(get_files $prefix)) # full record
  echo "Getting time series of energy terms."
  commands=("${climofiles[@]/#/ -selname,$energy_params }")
  cdo $flags -mergetime ${commands[@]} tmp.nc # merge the means

  # Get the autocorrelation
  # Have to be clever here -- no builtin function, but there is a builtin
  # timcor function; so select timesteps mismatched by one
  echo "Getting lag-1 autocorrelation from the time series."
  ntime=$(cdo ntime tmp.nc 2>/dev/null)
  timesteps=$(seq 1 $ntime | xargs | tr ' ' ',') # list of timesteps
  timesteps1=${timesteps#*,} # trim first timestep
  timestepsN=${timesteps%,*} # trim last one
  t=$(date +%s)
  cdo $flags -timcor -seltimestep,$timesteps1 tmp.nc \
    -seltimestep,$timestepsN tmp.nc $fautocorr
  rm tmp.nc # remove file
}

################################################################################
# Get single EOF from 2D latitude-pressure space
# Follows convention from Thompson paper
# This function is quite different from the 1D results:
# * Will generate two primary cross-section files: the autocorrelation, and the
#   projected eigenvectors. Attach some extra climate results (t, u) to both
#   of these files maybe. That way will be easier to plot them.
################################################################################
# Helper function that gets EOFs for single region and parameter
# In the future may want to interpolate to some standard low-horizontal
# resolution first. Consider this. Or don't, whatever.
eof2d_helper() {
  # Input
  [ $# -ne 2 ] && echo "Error: Bad arguments." && return 1
  region=$1
  param=$2
  neof=10              # number of EOFs
  tfilter="-daymean"   # consider using -selhour,0 or -daymean
  pfilter="-sellevidx" # filter pressure levels according to Thompson et al. methods
  selregion=$(region_selbox $region)
  plevs="$(cdo showlevel -selname,t ${climofiles[0]} 2>/dev/null)" # plev list
  ilev=1 # initialize
  for plev in $plevs; do # check em out
    if [ $(echo "$plev >= 200" | bc) -eq 1 ]; then
      pfilter+=",$ilev" # filter em yo
    fi
    ilev=$(($ilev+1))
  done
  # Message and determine filenames
  echo "Region ${region}." # echo level
  fseries=series${region}${param}.nc # name of time series
  feval=evals${region}${param}.nc # here, time dimension is EOF
  fevec=evecs${region}${param}.nc # here, time dimension
  feof=eofs${region}${param}.nc # mean projection of standardized PC onto data
  fpc=pcs${region}${param}.nc # here, 

  # First create complete time series of energy during 'climatology' timesteps
  # Note the pressure-level filter will preserve level bounds, because CDO is cool
  # if [ ! -r $fseries ]; then # do this once for NH and SH
  echo "Getting time series."
  commands=("${climofiles[@]/#/ $selregion $pfilter $tfilter -selname,$param }") # don't interpolate; keep it pure
  cdo $flags -mergetime ${commands[@]} $fseries
  # else echo "Time series for parameter ${param} already created."
  # fi

  # Next weight by pressure level thickness
  # NCAP is super neat and will broadcast across matching dimension names
  echo "Weighting pressure levels."
  ncap2 -O -v -s '*dp[$plev]=plev_bnds(:,1)-plev_bnds(:,0); '"${param}W=${param}"'*dp*100' \
    $fseries ${fseries%.nc}_weighted.nc; fseries=${fseries%.nc}_weighted.nc
  ncatted -O -a units,${param}W,o,c,"J/m2" $fseries # no longer per Pascal
  ncrename -v $param ${param%W} $ncfile &>/dev/null # chnage back the name the name

  # Next get first N EOF eigenvectors
  # For some reason Jacobi fails here; tried it out with many options
  # FNORM_PRECISION=1e-2 MAX_JACOBI_ITER=50 CDO_WEIGHT_MODE=on cdo $flags -eof3d,$neof \
  #   -setname,eof$param -detrend $fseries $feval $fevec # two output files
  echo "Getting EOFs."
  CDO_SVD_MODE=danielson_lanczos CDO_WEIGHT_MODE=on cdo -v -O -eof3d,$neof \
    -setname,eof$param -detrend $fseries $feval $fevec # two output files
  [[ ! -r $feval || ! -r $fevec ]] && echo "Error: Eigenvectors not produced." && return 1
  eofcommands1=() eofcommands2=()
  for ieof in $(seq 1 $neof); do
    eofcommands1+=("-setname,${param}_eof$(printf '%02d' $ieof) -seltimestep,$ieof $fevec")
    eofcommands2+=("-setname,${param}_eof$(printf '%02d' $ieof) -seltimestep,$ieof $feval")
  done
  echo "Expanding time dimension to individual variables."
  cdo $flags -merge ${eofcommands1[@]} tmp${param}${region}.nc; mv tmp${param}${region}.nc $fevec
  cdo $flags -merge ${eofcommands2[@]} tmp${param}${region}.nc; mv tmp${param}${region}.nc $feval
    # note these commands seem to raise errros, for whatever reason; the -merge
    # comand may detect duplicate variables before the individual -setname commands
    # are processed, so these errors can be safely ignored

  # Next get the PCs from the eigenvectors; eofcoeff not needed because computation is simple
  # Get dot product of area-weighted time series grid with EOF grid, then standardize time series
  # * Creates one file for each EOF, then merge them
  # * Remember pressure levels still weighted, so just use vertsum.
  echo "Getting PCs."
  pccommands=() # initialize
  for ieof in $(seq 1 $neof); do # see: https://stackoverflow.com/a/169517/4970632
    pccommand="-setname,${param}_pc$(printf '%02d' $ieof) -divc,101325 -vertsum -fldsum \
      -mul \
        -mul $fseries -gridweights $fevec \
        -selname,${param}_eof$(printf '%02d' $ieof) $fevec" # gets dot-product of weighted data with eigenvector
    pccommands+=("$pccommand") # add to list
  done
  echo "Standardizing PCs."
  cdo $flags -merge ${pccommands[@]} $fpc # put the PCs into one file, each one as separate variable
  cdo $flags -div -sub $fpc -timmean $fpc -timstd $fpc \
      ${fpc%.nc}std.nc; mv ${fpc%.nc}std.nc $fpc # mean 0, stdev 1
  ! [ -r $fpc ] && echo "Error: PC time series not produced." && return 1

  # Finally, get EOFs in physical units by multiplying the standardized
  # values by the original *unweighted* data; result is "anomaly associated with 1 stdev
  # variation of the PC time series"; make sure to use *unweighted* pressure levels too
  # * Name the initial variables ${param}W for weighted; then put them back
  echo "Projecting standardized PCs onto data."
  eofcommands=()
  for ieof in $(seq 1 $neof); do
    eofnum=$(printf '%02d' $ieof)
    eofcommands+=("-setname,${param}_eof${eofnum} -timmean \
    -mul -enlarge,$fevec -selname,${param}_pc${eofnum} $fpc \
          ${fseries%_*}.nc") # project onto *unweighted* time series
  done
  cdo $flags -merge ${eofcommands[@]} $feof # merge individual files
  ! [ -r $feof ] && echo "Error: Projection not created." && return 1
  return 0 # nice return value if got here
}

control_eof2d() {
  # Get files
  prefix=$psummary
  climofiles=($(get_files $prefix)) # full record
  # Next execute EOF function for a bunch of levels
  # Get the eigenvectors, eigenalues, PCs, and EOFs in physical units
  processes=() # check if shit went okay
  for region in NH SH; do
    # Get all EOF params
    logfiles=() # start here
    for param in KM KE; do # barotropic and baroclinic modes
      logfile=${log}2d${param}${region}
      echo "Getting ${param} EOFs and PCs for ${region}." | tee $logfile
      if $debug; then # no background stuff!
        eof2d_helper $region $param
      else
        eof2d_helper $region $param &>$logfile & # send to background, every time
      fi
      processes+=($!) # save process ID
      logfiles+=($logfile)
    done
  done # wait out here
  check_exits ${processes[@]}
  cp ${logfiles[0]} ${log}2deofs_get; rm ${logfiles[@]}

  # Do post-processing; this will have 4 threads running in parallel,
  # which is a decent number. Merges stuff.
  # * TODO: Consider using taskset to assign specific cores; maybe it really
  #   is faster. Gotta do it. Really.
  logfiles=()
  for region in NH SH; do
    # Merge the KM and KE files; variables will have different names
    for prefix in evecs evals pcs eofs; do
      mergelog=${plog}2d${prefix}${region}
      outfile=${filename}_2d${prefix}${region}.nc
      echo "Merging ${prefix%s} files." | tee $mergelog
      mergefiles=(${prefix}${region}??.nc)
      [ ${#mergefiles[@]} -eq 0 ] && echo "Error: Couldn't find any ${prefix%s} files." && exit 1
      cdo $flags -merge ${mergefiles[@]} $outfile &>$mergelog &
      processes+=($!)
      logfiles+=($mergelog)
    done
  done # wait out here
  check_exits ${processes[@]}
  cat ${logfiles[@]} > ${plog}2deofs_merge
  rm ${logfiles[@]}

  # Cleanup
  rm series*.nc evecs*.nc evals*.nc pcs*.nc eofs*.nc # delete dummy files
}

################################################################################
# Get EOFs and PCs of zonal-mean data
################################################################################
# Declare helper function that gets EOF from individual level
eof1d_helper() {
  # Initial stuff
  tfilter=-daymean # consider using -selhour,0 or -daymean
  neof=10 # number of EOFs
  region=$1 # the region
  level=$2 # the level
  param=$3 # parameter to EOFize
  selregion=$(region_selbox $region)

  # Message and determine filenames
  echo "Region ${region}; Level ${level}." # echo level
  printlevel=$(printf "%04d" $level)
  fseries=series${region}${printlevel}.nc # name of time series
  feval=evals${region}${printlevel}.nc # here, time dimension is EOF
  fevec=evecs${region}${printlevel}.nc # here, time dimension
  feof=eofs${region}${printlevel}.nc # mean projection of standardized PC onto data
  fpc=pcs${region}${printlevel}.nc # here, 

  # First create complete time series of wind during our 'climatology' timesteps (after spinup)
  # Just selects name, interpolates to single level, *then* interpolates in space
  if [ ! -r $fseries ]; then # do this once for NH and SH
    echo "Getting time series."
    if [ ${#climofiles[@]} -gt 1 ]; then mergetime="-mergetime"; else unset $mergetime; fi
    commands=("${climofiles[@]/#/ $selregion -intlevel,$level $tfilter -selname,$param }")
    cdo $flags $mergetime ${commands[@]} $fseries
  else echo "Time series for level $level already created."
  fi

  # Next get the PCs from the eigenvectors; eofcoeff not needed because computation is simple
  # Get dot product of area-weighted time series grid with EOF grid, then standardize time series
  # Creates one file for each EOF, then merge them
  echo "Getting PCs."
  pccommands=() # initialize
  for ieof in $(seq 1 $neof); do # see: https://stackoverflow.com/a/169517/4970632
    icommand="-setname,${param}_pc$(printf '%02d' $ieof) -fldsum \
      -mul \
        -mul $fseries -gridweights $fevec \
        -seltimestep,$ieof $fevec" # gets dot-product of weighted data with eigenvector
    pccommands+=("$icommand") # add to list
  done
  cdo $flags -merge ${pccommands[@]} $fpc # merge results of the multiplications
  cdo $flags -setname,$param -div -sub $fpc -timmean $fpc -timstd $fpc \
      ${fpc%.nc}std.nc; mv ${fpc%.nc}std.nc $fpc # sub mean, divide by std
      # important not to name something like tmp.nc or other background processes
      # may overwrite it in process
  [ ! -r $fpc ] && echo "Error: PC time series not produced." && return 1

  # Finally, get EOFs in physical units by multiplying the standardized
  # values by the original data; result is "anomaly associated with 1 stdev
  # variation of the PC time series"
  echo "Projecting standardized PCs onto data."
  eofcommands=() # initialize
  for ieof in $(seq 1 $neof); do # see: https://stackoverflow.com/a/169517/4970632
    icommand="-timmean -mul -enlarge,$fevec $fpc $fseries"
    icommand="-setname,eof$param$(printf '%02d' $ieof) -fldsum \
      -mul \
        -mul $fseries -gridweights $fevec \
        -seltimestep,$ieof $fevec" # gets dot-product of weighted data with eigenvector
    eofcommands+=("$icommand") # add to list
  done
  cdo $flags -timmean -mul -enlarge,$fevec $fpc $fseries \
    $feof # mean projection of standardized PC onto data
  [ ! -r $feof ] && echo "Error: Projection not created." && return 1
  return 0 # nice return value always
}

control_eof1d() {
  # Initial stuff
  climofiles=($(get_files $psummary)) # full record
  # levels=({5..10}00) # range of pressure levels in mb
  levels=(50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000)

  # Next execute EOF function for a bunch of levels
  # Get the eigenvectors, eigenalues, PCs, and EOFs in physical units
  for param in u KM KE; do
    for region in NH SH; do
      # Get EOF for ***each level***
      # Will generate logfiles for each process, but only save ***one*** of them
      # as a sample
      # Will also check exit statuses of each process
      processes=()
      logfiles=()
      for level in ${levels[@]}; do
        logfile=${plog}eofs${region}$(printf "%04d" $level)
        echo "Getting EOFs and PCs for ${region}; ${level}mb." | tee $logfile
        eof1d_helper $region $level $param &>$logfile & # send to background, every time
        processes+=($!) # save process ID
        logfiles+=($logfile) # add to list of logfiles
      done #; wait
      check_exits ${processes[@]}
      # cat ${logfiles[@]} >${plog}eofs${region}; rm ${logfiles[@]}
      cp ${logfiles[0]} ${plog}eofs_get; rm ${logfiles[@]}

      # ***Merge separate eof levels*** into single file
      # CDO by default will combine vars with identical names along level axis, if
      # their levels differ.
      logfiles=()
      for prefix in evecs evals pcs eofs; do
        logfile=${plog}merge-${prefix}${region}
        outfile=${filename}_${prefix}${region}.nc
        echo "Merging ${prefix%s} files." | tee $logfile
        mergefiles=(${prefix}${region}[0-9][0-9][0-9][0-9].nc) 
        [ ${#mergefiles[@]} -eq 0 ] && echo "Error: Couldn't find any ${prefix%s} files." && exit 1
        cdo $flags -merge ${mergefiles[@]} $outfile &>$logfile &
        processes+=($!) # save process ID
        logfiles+=($logfile)
      done; wait # then wait
      check_exits ${processes[@]}
      # mv ${logfiles[0]} ${plog}eofs${region}; rm ${logfiles[@]}
      cat ${logfiles[@]} >${plog}eofs_merge; rm ${logfiles[@]}
    done
  done

  # Cleanup
  # tail -vn +1 ${plog}eof?* &>${plog}eof # from https://stackoverflow.com/a/7816490/4970632
  # rm ${plog}eof?* # remove inidivudal logfiles after combining them
  rm series*.nc evecs*.nc evals*.nc pcs*.nc eofs*.nc # delete dummy files
}

################################################################################
# Spindown stuff
################################################################################
# Create ensemble-mean latitude cross-section of spindown, with time axis preserved
# * Old approach had us creating massive time-merged files of each spindown run, then
#   taking the ensemble mean of each massive file
# * New approach just has us get the ensemble mean of each day-range, then merge the time
#   axis of the small group of ensemble means
# * Each iteration of loop in new approach takes about as long as iterations from old approach (a couple minutes),
#   and the final step doesn't hang anymore (not sure if it ever would have finished).
spindown_ensemble_mean_ts() {
  # Verify files present
  spindown_files_by_day
  # First determine unique groups of spindown days
  count= # start as empty
  outfiles=() # save temporary files
  echo "Getting ensemble mean from ${#spindowndayfiles[@]} groups."
  for spindowngroup in "${spindowndayfiles[@]}"; do
    # First run a simple check
    spindowngroup=($spindowngroup) # the space-separated list of files is now expanded into an array
    newcount=${#spindowngroup[@]}
    members=(${spindowngroup[@]##*/${psummary}.})
    members=(${members[@]%%-*})
    [ ! -z $count ] && [ $count != $newcount ] && \
      echo "Error: $newcount spindown runs in this group, but $count files in previous groups." && exit 1
    count=$newcount
    # Next get ensemble mean of files; timesteps will be adopted from the first input file
    daystring=${spindowngroup[0]#$input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-}
    daystring=${daystring%.nc}
    outfile=$input/spindown${daystring}.nc
    echo "Spindown runs in the ${daystring} group: ${members[@]}."
    cdo $flags -ensmean ${spindowngroup[@]} $outfile &>/dev/null # log not necessary here
    outfiles+=($outfile)
  done; wait # wait for everything
  # From results, get ensemble mean of full spindown process
  echo "Merging the ensemble means: ${outfiles[@]##*/}."
  cdo $flags -mergetime ${outfiles[@]} $fspinclimate
  rm ${outfiles[@]}
}

# Create files with 'record' dimension showing global-average and polar-average
# spindown process for every branched spindown run
# Use CDO ngrids to create temporary fix for files with globally averaged values
spindown_regional_mean_ts() {
  # Initial stuff
  counter=0 # counter for waiting
  spindown_files_by_init
  echo "Creating records of individual spindown runs."

  # Get time-averages of spindown files from each starting date
  for region in globe polenh polesh; do
    outfiles=()
    spindowndays="" # empty string
    echo "Average over ${region}."
    for spindowngroup in "${spindowntsfiles[@]}"; do
      # Initial stuff
      counter=$(($counter+1))
      spindowngroup=($spindowngroup) # the space-separated list of files is now expanded into an array
      spindownday=${spindowngroup[0]#$input/${psummary}.}
      spindownday=${spindownday%%-*} # the spindown day
      echo "Files in ${spindownday} run: ${spindowngroup[@]##*spindown?-}."
      # Create spindown files
      # * Accomadate old files with two grids (global energy budget variables,
      #   and normal latitude-slice variables)
      # * Beware very strange issue; if combine selgrid with sellevidx/seltimestep, with the
      #   latter coming after sellonlatbox, get error 'longitude dimension is too small'; BUG
      selregion=$(region_selbox $region)
      outfile=$input/spindown${spindownday#d}${region}.nc
      commands=("${spindowngroup[@]/#/ -fldmean $selregion -selgrid,1 }") # average
      cdo $flags -mergetime ${commands[@]} $outfile &>/dev/null & # log not necessary here
      [ $(($counter % 10)) -eq 0 ] && wait # only do a handful at a time
      # [ $counter -eq 2 ] && wait # only do a handful at a time
      # [ $counter -eq 2 ] && break # testing
      outfiles+=($outfile) # add outfile
      spindowndays+="${spindownday#d}," # add spindown day
    done; wait

    # From results, create ensemble file of spindown process
    # CDO can't read 5-D files so no more CDO processing hereafter
    echo "Getting ensemble record of spindown runs from files: ${outfiles[@]##*/}."
    ensfile=${filename}_spindown${mode}${region}.nc
    ncecat -O -u member ${outfiles[@]} $ensfile
    ncks -O -4 --fix_rec_dmn member $ensfile $ensfile
    ncks -O -4 --mk_rec_dmn time $ensfile $ensfile
    ncap2 -O -s "member[\$member]={${spindowndays%,}}" $ensfile $ensfile
    ncatted -O -a long_name,member,o,c,"day of initiation from control run branch" \
               -a units,member,o,c,"days since 0000-00-00 00:00:00" $ensfile
    ncks -O $ensfile $ensfile # alphabetize output
      # fixes record dimension; see https://sourceforge.net/p/nco/bugs/85/
      # the -4 is needed or an error is thrown, weirdly
    rm ${outfiles[@]}
  done

  # Special treatment where we want to average cross-sections from each hemisphere
  # Maybe modify this maybe
  file1=(${filename}_*polenh.nc) file2=(${filename}_*polesh.nc)
  [[ ${#file1[@]} != 1 || ${#file2[@]} != 1 ]] && echo "Error: Had issues averaging poles together." && exit 1
  ncea $file1 $file2 $fspindownpoles
  rm $file1 $file2 # each pole should be thought of as additional ensemble member; so 50 runs == 100
}

# Timescale stuff
dynamical_timescale() {
  echo "Esimating dynamical timescale."
  ! [ -r $fspindownpoles ] && echo "Error: Spindown file over poles $fspindownpoles is not available."
  ! [ -r $fclimate ] && echo "Error: The climate file $fclimate is not available."
  # Python approach due to illegible ensemble data
  # python3 -c | tee ${plog}timescale << EOF
  import=postprocess_funcs # name of module
  python3 << EOF
import os
os.chdir("$cwd") # go to scripts directory
import $import as timescales # import function
timescales.timescales(spindown="$fspindownpoles", climate="$fclimate", output="$ftimescale",
  timestep=slice(-4*100,None)) # use the last 100 days for equilibrium state
EOF
  ncks -O -x -v lon,lat,time $ftimescale $ftimescale # drop vars and alphabetize
  # Simple CDO approach for the dynamical timescale
  # cp $fclimate tmp.nc
  # ncwa -O -a lon tmp.nc tmp.nc
  # cdo -ensmean -fldmean -sellonlatbox,-180,180,-90,-60 -selname,ndamp $fclimate \
  #              -fldmean -sellonlatbox,-180,180,60,90 -selname,ndamp $fclimate \
  #     reference.nc
  # cdo -setname,timescale -mulc,3600 -mulc,24
  # ncatted -O -a units,timescale,o,c,"days" \
  #            -a long_name,timescale,o,c,"dynamical timescale"
}

# Main function for applying post-processing
driver() {
  # Climate means
  if $spectral_climate; then
    echo "Average phase-speed spectra."
    log=${plog}control_climate
    call_parallel spectral_climate
  fi
  if $control_climate; then
    echo "Control climate."
    log=${plog}control_climate
    call_parallel control_climate $psummary $fclimate
  fi
  if $control_climate_isen; then
    echo "Isentropic control climate."
    log=${plog}control_climate
    call_parallel control_climate $psummary_isen $fclimate_isen
  fi
  # NH/SH regional means, no averaging in pressure-space
  if $control_regional_ts; then
    echo "Control regional-mean time series."
    log=${plog}timeseries
    call_parallel control_regional_ts $psummary
  fi
  if $control_regional_ts_isen; then
    echo "Isentropic control regional-mean time series."
    log=${plog}timeseries
    call_parallel control_regional_ts $psummary_isen
  fi
  # Climate autocorrelation
  if $control_energy_rho; then
    echo "Control global energy budget autocorrelation."
    log=${plog}control_energy_rho
    call_parallel control_energy_rho
  fi
  # Climate time series
  if $control_energy_ts; then
    echo "Control global energy budget time series."
    log=${plog}control_energy_ts
    call_parallel control_energy_ts
  fi

  # Climate variability: single-level zonal wind EOF
  if $control_eof1d; then
    echo "Control EOFs of zonal wind."
    log=${plog}eofs
    call_parallel control_eof1d
  fi
  # Climate variability: energy EOF
  if $control_eof2d; then
    echo "Control 2D EOFs of energy terms."
    log=${plog}2deofs
    call_parallel control_eof2d
  fi

  # Spindown processing
  # Get the full cross-section
  if $spindown_ensemble_ts; then
    echo "Spindown ensemble mean time series."
    log=${plog}spindown${mode}
    call_parallel spindown_ensemble_mean_ts
  fi
  # Preserve each record but take area averages
  if $spindown_regional_ts; then
    echo "Spindown regional average time series for each branch."
    log=${plog}spindown$mode
    call_parallel spindown_regional_mean_ts
  fi
  # Timescale
  if $spindown_timescale; then
    echo "Spindown dynamical timescale."
    log=${plog}spindown$mode
    call_parallel dynamical_timescale
  fi
}
# Call driver!
# Nice to organize it this way, so the final block of code shows up in ctags
# NOTE: First we copy over forcing data
if [ -r $input/../forcing.nc ]; then
  echo "Copying forcing data."
  cp $input/../forcing.nc $output
fi
driver
exit 0

################################################################################
# # Get ensemble-statistics for equilibrium control files
# # Also get the selected difference for given days, and statistics
# function Diffs() {
#   region=$1
#   tspindown1=$2
#   tspindown2=$3
#   if [ -z $region ] || [ -z $tspindown1 ] || [ -z $tspindown2 ]; then
#     echo "Error: Must supply Diffs() function with region and times you wish to compare."
#     return 1
#   fi
#   # Get list of files
#   shopt -s nullglob
#   file1=${fequilibrium%.nc}-$tspindown1$region.nc
#   file2=${fequilibrium%.nc}-$tspindown2$region.nc
#   files=(${fequilibrium%.nc}-[0-9][0-9][0-9][0-9]$region.nc)
#   if [ -z $files ]; then
#     echo "WARNING: Could not find any equilibrium control files."
#     return 1
#   fi
#   echo $file1
#   echo $file2
#   if [ ! -r $file1 ] || [ ! -r $file2 ]; then
#     echo "WARNING: Could not find the selected equilibrium control files."
#     return 1
#   fi
#   # Our selected difference
#   echo "First the selected difference."
#   cdo $flags -sub $file1 $file2 ${fequilibrium%.nc}-diff$region.nc
#   # Bootstrap the differences
#   echo "Next bootstrap random differences, and get the standard deviation."
#   length=${#files[@]}
#   for i in {1..1000}; do
#     ch1=$(expr $RANDOM % $length) # random int32, find remainder
#     ch2=$(expr $RANDOM % $length)
#     while [ $ch1 -eq $ch2 ]; do
#       ch2=$(expr $RANDOM % $length) # repeat until two DIFFERENT choices
#     done
#     cdo $flags -sub ${files[$ch1]} ${files[$ch2]} boot$(printf "%04d" $i)$region.nc
#   done
#   # Just get every single possible difference instead
#   # echo "Next get the difference of every single possible pair, and percentiles."
#   # maxid=$(expr ${#files[@]} - 1) # maximum ID is length - 1
#   # for ch1 in $(seq 0 $maxid); do
#   #   for ch2 in $(seq $(expr $ch1 + 1) $maxid); do
#   #     cdo $flags sub ${files[$ch1]} ${files[$ch2]} \
#   #       boot$(printf "%02d" $ch1)$(printf "%02d" $ch2)$region.nc
#   #   done
#   # done
#   # Standard deviation and percentiles of differences
#   files=(boot[0-9][0-9][0-9][0-9]$region.nc) # will delete these later
#   cdo $flags -ensstd ${files[@]} ${fequilibrium%.nc}-diffstd$region.nc
#   if [ -x /usr/bin/cdo ]; then # anaconda version still uses integer params
#     /usr/bin/cdo -s -O -enspctl,2.5 ${files[@]} ${fequilibrium%.nc}-difflo$region.nc
#     /usr/bin/cdo -s -O -enspctl,97.5 ${files[@]} ${fequilibrium%.nc}-diffhi$region.nc
#       # apparently if try to use -P flag, will NOT EXECUTE IN PARALLEL
#   else echo "WARNING: Could not find CDO version with float-parameter percentile."
#   fi
#   rm ${files[@]} # remove the helper files
# }
################################################################################
# For EOF stuff
# To get the approximate degrees of freedom ('independent samples') for a
# given field (in this case energy, for an example), divide the square
# of the trace of temporal covariance matrix by one half of this variance.
# See Bretherton et al. 1999, Equation 5
# Returns the trace of the full covariance matrix; i.e. covariance in space
# of the horizontal field at each timestep, for each level.
# TODO: We actually want the 3d covariance, not 2d?
# TODO: This should not be necessary right? Should just get this stuff
# directly in EOF scripts? 
################################################################################
# fcovar=${filename}_covar.nc
# energy_budget_covar() {
#   # Files
#   climofiles=($(get_files $pclimate))
#   # Get the single time-series file
#   echo "Getting time series of energy terms through days ${climofiles[@]##*/${psummary}.}."
#   commands=("${climofiles[@]/#/ -selname,$energy_params }")
#   cdo $flags -mergetime ${commands[@]} tmp.nc # merge the means
#
#   # Now get the covariance matrix trace; note that fldcovar just gets the
#   # area-weighted sum of each point dotted with itself
#   # * The area-weight is applied as Sum(w*(i1-i1mean)*(i2-i2mean)), so to weight
#   #   by pressure-level, just take vertmean after the fact.
#   # * Sums over fields at each level were *partial* sums; by taking vertical mean,
#   #   we just *continue* the partial sum over the rest of the 3D field
#   cdo $flags -vertmean -fldcovar tmp.nc tmp.nc $fcovar
#   rm tmp.nc
# }

