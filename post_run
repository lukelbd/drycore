#!/bin/bash
################################################################################
# This script post-processes the raw model output files generated from the
# GFDL dry core model. Needs to be supplied with a source and destination directory.
# WARNING: Destination directory should really be on a backed up disk.
# WARNING: CDO has hard-coded maximum chained processes of 64. Horrible. Use
# commands like 'select,key=value' to merge multiple files in one command instead
# of constructing massive chained commands in single bash string.
################################################################################
# Raise error and exit
shopt -s nullglob # will return empty if no match
raise() {
  echo "Error: $@" 1>&2
  exit 1
}
# Python heredoc header
header="
import time
t1 = time.time()
t0 = t1
def timer(message):
    global t1
    t2 = time.time()
    print(f'{message}: {t2-t1:.3f}s')
    t1 = t2
"
# Environmental variables, with defaults
# The flag names should be descriptive enough, but feel free to read comments
flags='-s -O' # overwrite, and only issue warnings
netcdf=true # use subfolder? have this so if we just got forcing data and have no real data yet
climo_start=500
climo_end=1500
forcing=false
climate_plev=false
climate_isen=false
climate_lorenz=false
climate_autocorr=false
climate_variability=false
climate_spectral=false
# Scripts
f2c=$(pwd)/post_f2c.py # converts frequency-wavenumber data to phase speed-wavenumber space
eof=$(pwd)/post_eofs.py
dyn=$(pwd)/post_dyntau.py
# Spindown options
spin_start=0
spin_end=100000
spindown_timescale=false
spindown_ensemble_ts=false
spindown_regional_ts=false
# Parse user input
mode=2 # spindown mode
while [ $# -gt 0 ]; do
  case $1 in
    # Params
    -m=*) mode=${1##*=} ;;
    -r=*) rdays=${1##*=} ;;
    -c1=*) climo_start=${1##*=} ;;
    -c2=*) climo_end=${1##*=} ;;
    -s1=*) spin_start=${1##*=} ;;
    -s2=*) spin_end=${1##*=} ;;
    # Flags
    -f) forcing=true ;;
    -cp) climate_plev=true ;;
    -cs) climate_spectral=true ;;
    -ci) climate_isen=true ;;
    -ce) climate_lorenz=true ;;
    -cv) climate_variability=true ;; # variability
    -ca) climate_autocorr=true ;;
    -se) spindown_ensemble_ts=true ;;
    -sr) spindown_regional_ts=true ;;
    -st) spindown_timescale=true ;;
    -*) raise "Unknown flag \"${1}\".";;
    *) # 3 required arguments, set in this order
      if [ -z $filename ]; then
        filename="$1"
      elif [ -z $input ]; then
        input="$1"
      elif [ -z $output ]; then
        output="$1"
      else raise "Too many arguments."
      fi ;;
  esac
  shift
done
# Require restart days because may have some old 5-day tests or whatever
# in that experiment folder.
[ -z "$rdays" ] && raise "You must specify the restart day spacing with -r=N or --restart=N."
# Requirements
if $netcdf; then
  ! [ -d "$input/netcdf" ] && [ -d "$input" ] && mkdir "$input/netcdf"
  input="$input/netcdf"
fi
if [ -z "$filename" ] || [ -z "$input" ] || [ -z "$output" ]; then
  raise "Need filename prefix (e.g. 2xdaily_inst), input folder, and output folder, in that order."
fi

# File management/names
# Prefixes for input files
pfull=${filename}_full
psummary=${filename}_summary # original prefix file names
peofs=${filename}_eofs
pseries=${filename}_series
pspectral=${filename}_spectral
pfull_isen=${filename}_full_isen
psummary_isen=${filename}_summary_isen
# Destinations for output files
days="d$(printf "%04d" ${climo_start})-d$(printf "%04d" ${climo_end})"
fspectral=${filename}_spectral.${days}.nc
fautocorr=${filename}_autocorr.${days}.nc
ftimescale=${filename}_autocorr_timescale.${days}.nc
fclimate=${filename}_climate.${days}.nc
fclimate_isen=${filename}_climate_isen.${days}.nc
fregion=${filename}_timeseries.${days}.nc
fregion_isen=${filename}_timeseries_isen.${days}.nc
fenergy=${filename}_energy.${days}.nc
fspinclimate=${filename}_spindown${mode}xs.${days}.nc # cross-section
fspindownpoles=${filename}_spindown${mode}poles.${days}.nc # spindown rate at the poles
fdyntimescale=${filename}_dynamical_timescale${mode}.${days}.nc # the timescale stuff

# Directory management; move to save directory
cwd=$(pwd) # directory where scripts stored (you must run this script from current directory!)
! [ -d "$input" ] && raise "Cannot find input directory \"$input\"."
! [ -d "$output" ] && mkdir $output
cd $output # move here

# Energy terms
ts_params=ke,km,pe,pm,ehf,emf
energy_params=ckekm,cpeke,cpmkm,cpmpe,dke,dkm,gpe,gpm,ke,km,pe,pm

################################################################################
# Helper function, checks if destination file is newer than source file
################################################################################
# Check if destination file is *newer* than source files
newer() {
  local ret src file date idate
  dest="$1"
  src=("${@:2}")
  if [ ${#src[@]} -eq 0 ]; then
    echo "Skipping (source file(s) unavailable)." 1>&2
    return 0
  fi
  # If version without day string exists, remove it
  idest="${dest%%.*}.nc"
  if [ "$dest" != "$idest" ] && [ -r "$idest" ]; then
    rm "$idest"
    echo "Warning: Removed file \"$idest\" with no day string."
  fi
  # Test if sources are available; never try to run processing script if not
  for file in "${src[@]}"; do
    if ! [ -r "$file" ]; then # TODO: test with ncdump? nah.
      echo "Skipping (source file(s) un-readable)." 1>&2
      return 0 # do not re-process since a source is unavailable!
    fi
  done
  # Check if destination exists
  if ! [ -r "$dest" ]; then
    echo "Running (${dest##*/} not found)." 1>&2
    return 1
  fi # re-process since destination is unavailable!
  # Finally test dates, and check for curropt files
  date=$(date +%s -r "$dest" 2>/dev/null) # just get this once
  for file in "${src[@]}"; do
    idate=$(date +%s -r "$file" 2>/dev/null)
    if [ "$idate" -gt "$date" ]; then
      echo "Running (${dest##*/} older than source file(s))." 1>&2
      return 1 # destination is *not* newer, re-process!
    fi
  done
  # Test if file valid
  # WARNING: Common error is length-zero time dimension
  ncdump -h "$dest" &>/dev/null # if 1, destination is *corrupt*, re-process!
  if [ $? -ne 0 ]; then
    echo "Running (${dest##*/} appears corrupt)." 1>&2
    return 1
  fi
  ntime=$(ncdump -h "$dest" | grep UNLIMITED | tr -dc '[0-9]') # format should be 'time = UMLIMITED; // (0 currently)'
  if [ -n "$ntime" ] && [ $ntime -eq 0 ]; then
    echo "Running (${dest##*/} has zero-length time dim)." 1>&2
    return 1
  fi
  # Final
  echo "Skipping (${dest##*/} exists and is new)." 1>&2
  return 0
}

################################################################################
# NetCDF utils copied from bashrc
################################################################################
nclist() {
  command ncdump -h "$1" | sed -n '/variables:/,$p' | sed '/^$/q' | grep -v '[:=]' \
    | cut -d '(' -f 1 | sed 's/.* //g' | xargs | tr ' ' '\n' | grep -v '[{}]' | xargs
}
ncdimlist() { # get list of dimensions
  command ncdump -h "$1" | sed -n '/dimensions:/,$p' | sed '/variables:/q' \
    | cut -d '=' -f 1 -s | xargs | tr ' ' '\n' | grep -v '[{}]' | xargs
}
ncvarlist() { # only get text between variables: and linebreak before global attributes
  local list=($(nclist "$1"))
  local dmnlist=($(ncdimlist "$1"))
  local varlist=() # add variables here
  for item in "${list[@]}"; do
    if [[ ! " ${dmnlist[@]} " =~ " $item " ]]; then
      varlist+=("$item")
    fi
  done
  echo "${varlist[@]}" | tr -s ' ' '\n' | grep -v '[{}]' | xargs # print results
}
ncin() {
  nclist $1 | grep $2 &>/dev/null
}
ncvardump() {
  command ncdump -v "$1" "$2" | tac | \
    grep -E -m 1 -B100 "[[:space:]]$1[[:space:]]" | sed '1,1d' | tac | \
    tr -d ',;'
}
ncdims() {
  command ncdump -h "$1" | grep -B100 "^variables:$" | sed '1,2d;$d' | tr -d ';' | tr -s ' ' | column -t | less
}

################################################################################
# Helper functions
################################################################################
# Wait for processes from array of process IDs, and check their exit statuses
# WARNING: No longer do this! Just parallelize with post_series.
parallel() {
  # $@ &>$log & # send to background, save logfile
  t=$(date +%s)
  $@ # just naked call to command
  echo "Elapsed time: $(($(date +%s) - t))s."
}
check() {
  local i pids stats pid ex
  pids=($@)
  for pid in ${pids[@]}; do
    wait $pid
    stats+=($?) # if process already done, wait just mimicks its exit status
  done
  i=0 # iterate
  for stat in ${stats[@]}; do
    if [ $stat -ne 0 ]; then
      raise "One of the background processes failed: \"$(ps -p ${pids[i]} -o comm=)\"."
    fi
    let i+=1
  done
}
# Select region
selregion() {
  region="$1"
  case $region in
    [nN][hH])     selregion="-sellonlatbox,0,0,20,70"   ;; # north-hemisphere selection
    [sS][hH])     selregion="-sellonlatbox,0,0,-70,-20" ;; # south-hemisphere selection
    pole[nN][hH]) selregion="-sellonlatbox,0,0,60,90"   ;;
    pole[sS][hH]) selregion="-sellonlatbox,0,0,-90,-60" ;;
    globe)        selregion="-sellonlatbox,0,0,-90,90" ;;
    *) raise "Invalid region ${region}." && return 1 ;;
  esac
  echo $selregion
}
# Print list of days given array
# Just for convenience
print_days() {
  unset days
  for file in $@; do
    daystring=${file##*/}
    daystring=${daystring%.nc}
    daystring=${daystring#*.}
    day1=${daystring%%-*}
    day1=${day1#d}
    day2=${daystring##*-}
    day2=${day2#d}
    days+=($(printf "%.0f" $day1))
  done
  echo ${days[@]}
}

################################################################################
# Get lists of files
################################################################################
# Climate files within day range (e.g. to ignore spinup)
# Used for climate means and climate variability stuff
# climate_files() {
get_files() {
  # Glob
  local prefix file files ffiles start end vars
  local daystring nfiles day1 day2
  prefix=$1
  start=$climo_start # the *default* starting/ending points
  end=$climo_end
  [ -n "$2" ] && start=$2
  [ -n "$3" ] && end=$3
  files=($input/${prefix}.d????-d????.nc)
  [ ${#files[@]} -eq 0 ] && raise "No $prefix files found in ${input}."
  # Checks
  nfiles=$((($end - $start) / $rdays)) # expectation
  [ $((($end - $start) % $rdays)) -ne 0 ] && \
    raise "Non-integer number of $rdays-day blocks between days $start and $end."
  for file in ${files[@]}; do
    # Get start and end days
    daystring=${file##*/}
    daystring=${daystring%.nc}
    daystring=${daystring#*.}
    day1=${daystring%%-*}
    day1=$(printf "%.f" ${day1#d})
    day2=${daystring##*-}
    day2=$(printf "%.f" ${day2#d})
    if [ $((day2 - day1)) -ne $rdays ]; then
      continue
    fi
    # Add to list on condition
    if [ $day1 -ge $start ] && [ $day2 -le $end ]; then
      ffiles+=($file)
    fi
  done
  # Message and exit
  if [ ${#ffiles[@]} -eq 0 ]; then
    raise "No $prefix files found between days $start and ${end}."
  elif [ ${#ffiles[@]} -ne $nfiles ]; then
    raise "Expected $nfiles files between days $start and ${end}, got ${#ffiles[@]}."
  else
    echo "Found all files between days $start and ${end}." 1>&2
    echo ${ffiles[@]} # intended for user to capture output
  fi
}

# Spindown files grouped by initialization (branch) day
# Used for determining if spindowns are 'unique'
# TODO: Better print message
spindown_files_by_init() {
  echo "Getting list of spindown files within days $spin_start to $spin_end, grouped by control run initiation day."
  startdays=()
  for file in $input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-d[0-9][0-9][0-9][0-9]-d[0-9][0-9][0-9][0-9].nc; do
    startday=${file#$input/${psummary}.}
    startday=${startday%%-*} # the spindown day
    startdays+=($startday)
  done
  startdays=($(echo "${startdays[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')) # unique days
  spindowntsfiles=()
  for startday in ${startdays[@]}; do
    spindowngroup="" # for particular days
    for file in $input/${psummary}.${startday}-spindown$mode-d[0-9][0-9][0-9][0-9]-d[0-9][0-9][0-9][0-9].nc; do
      daystring=${file#$input/${psummary}.${startday}-spindown$mode-} # trim leading pattern
      daystring=${daystring%.nc}
      day1=${daystring%%-*} day1=${day1#d}
      day2=${daystring##*-} day2=${day2#d}
      [ $day1 -ge $spin_start ] && [ $day2 -le $spin_end ] && spindowngroup+=" $file"
    done
    spindowntsfiles+=("$spindowngroup") # the group of files is preserved as single array element separated by strings
  done
  [ ${#spindowntsfiles[@]} -eq 0 ] && raise "No $psummary files found between days ${spin_start} and ${spin_end}."
}

# Spindown files grouped by daystring (days after control run)
# Used for dynamical timescale stuff
# TODO: Better print message
spindown_files_by_day() {
  echo "Getting list of spindown files within days $spin_start to $spin_end, grouped by run day."
  daystrings=()
  for file in $input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-d[0-9][0-9][0-9][0-9]-d[0-9][0-9][0-9][0-9].nc; do
    daystring=${file#$input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-} # trim leading pattern
    daystring=${daystring%.nc}
    day1=${daystring%%-*} day1=${day1#d}
    day2=${daystring##*-} day2=${day2#d}
    [ $day1 -ge $spin_start ] && [ $day2 -le $spin_end ] && daystrings+=($daystring)
  done
  daystrings=($(echo "${daystrings[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')) # unique days
  spindowndayfiles=()
  for daystring in ${daystrings[@]}; do
    spindowngroup="" # for particular days
    for file in $input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-${daystring}.nc; do
      spindowngroup+=" $file"
    done
    spindowndayfiles+=("$spindowngroup") # the group of files is preserved as single array element separated by strings
  done
  [ ${#spindowndayfiles[@]} -eq 0 ] && raise "No $psummary files found between days ${spin_start} and ${spin_end}."
}
# echo "${spindowndayfiles[@]##*/}"
# for spindowngroup in "${spindowndayfiles[@]}"; do # testing
#   echo NEW GROUP; newgroup=(${spindowngroup}); echo ${newgroup[@]##*/${psummary}.}
# done

################################################################################
# Simple averages
################################################################################
# Control climate data, and reference to other control climates
# Notes:
# * Cannot chain the merge command; must be highest level no matter what. So
#   prepend timmean commands to files being merged.
# * For some reason doing -merge on chained/interpolated data results in duplicate
#   pressure level coordinates; interpolating them individually avoids this, and 
#   explicitly re-declaring the z-axis description also avoids this.
control_climate() {
  # Calculate with CDO
  prefix=$1
  out=$2
  files=($(get_files $prefix))
  newer $out ${files[@]} && return 0
  [ -r $out ] && rm $out
  echo "Getting time-means."
  commands=("${files[@]/#/ -timmean }")
  cdo $flags -ensmean ${commands[@]} $out
}

# Average spectral runs
# More complicated because no record dimension
# Use undocumented feature referenced here: https://sourceforge.net/p/nco/discussion/9830/thread/cee4e1ad/
# A 'negative' length indicates unlimited
# Tried using the documented feature NC_UNLIMITED: http://nco.sourceforge.net/nco.html#Dimensions
# but did not work. NOTE: No longer do this so forget it.
climate_spectral() {
  # Initial
  out=$fspectral
  # Merge with xarray and Dask
  # The fancy sed transforms 'a b c d' to '"a", "b", "c", "d", '
  files=($(get_files $pspectral))
  newer $out ${files[@]} && return 0
  echo "Getting average spectra."
  export PYTHONUNBUFFERED=1
  [ -r $out ] && rm $out
  python3 <<EOF
import os
import xarray as xr
$header
# Go
# TODO: Add support for windowing
print("Averaging spectra")
ds = xr.open_mfdataset([$(echo ${files[@]} | sed 's/[^ ][^ ]*/"&", /g')],
     concat_dim='record', chunks={'f':25}, decode_cf=False)
timer(" * Read time")
ds = ds.mean(dim='record', keep_attrs=True) # by default attrs thrown out for all computations, which makes sense
timer(" * Mean time")
if os.path.exists("$out"):
    os.remove("$out")
ds.attrs.pop('_NCProperties', None) # causes weird error if present!
ds.to_netcdf("$out")
timer(" * Save time")
EOF
  [ $? -ne 0 ] && raise "Merge failed."
  # Translate from k-frequency space to k-phase speed space
  out_phase=${out%.nc}_phase.nc
  # if ! [ -r $out_phase ]; then
  if true; then
    [ -r $out_phase ] && rm $out_phase
    t=$(date +%s)
    python3 $f2c $out $out_phase
    [ $? -ne 0 ] && raise "Translation failed."
    echo " * Time for translating to phase speeds: $(($(date +%s) - t))s."
  fi
}

################################################################################
# Autocorrelation
# Returns autocorrelation up to N lags, from which we can perform a best-fit
# and get a "timescale"
################################################################################
climate_autocorr() {
  # Just get the autocorrelation from the time-mean
  unset outs
  out=$fautocorr
  files=($(get_files $pfull))
  if ! newer $out ${files[@]}; then
    num=1
    nlag=25 # up to 25 days
    block=0 # block number
    ncorr=2 # process in groups of 2 files, or 200 days
    # Get correlation in blocks
    # Note the following was wrong:
    # mean = ds.mean(dim='time', keep_attrs=True)
    # var = ds.var(dim='time', keep_attrs=True)
    # corr = (((ds.isel(time=slice(i,None)) - mean) * (ds.isel(time=slice(None,-i)) - mean)).mean(dim='time') / var).mean(dim='lon')
    for file in "${files[@]}"; do
      ifiles+=("$file")
      if [ $num -ne $ncorr ]; then
        let num+=1
        continue
      fi
      iout=${out%.nc}.${block}.nc
      outs+=($iout) # record
      echo "Using files: ${ifiles[@]##*/}"
      stdbuf -oL -eL python3 << PYTHON
import os
import numpy as np
import xarray as xr
import climpy
$header
# Load files
# TODO: Change chunk size?
print("Getting autocorrelation")
ds = xr.open_mfdataset([$(echo ${ifiles[@]} | sed 's/[^ ][^ ]*/"&", /g')], concat_dim='time', decode_cf=False)
time_ = ds['time'] # cannot conflict with time module
dt = time_.values[1] - time_.values[0]
timer(" * Read time")
# Make output
dims = ('lag', 'plev', 'lat')
lag = np.arange(0, ${nlag} + dt/2, dt)
lag = xr.DataArray(lag, dims=('lag',), name='lag', attrs={'long_name':'lag', 'units':'days'})
out = xr.Dataset(coords={
    'lag':  lag,
    'lat':  ds['lat'],
    'plev': ds['plev'],
    })
longs = {
    't': 'temperature',
    'u': 'zonal wind',
    }
timer(" * Output time")
# Get autocorrelation
for var in ['t','u']:
    # data = ds[var].values
    data = ds[var].data
    _, corr = climpy.corr(data, dt=dt, nlag=${nlag}, axis=0)
    corr = corr.mean(axis=-1) # get mean along longitudes
    attrs = {'long_name':longs[var] + ' autocorrelation', 'units':'none'}
    out[var] = (dims, corr, attrs)
timer(" * Correlation time")
if os.path.exists("$iout"):
    os.remove("$iout")
out.to_netcdf("$iout")
timer(" * Save time")
PYTHON
      [ $? -ne 0 ] && raise "Autocorr calculation failed."
      # Reset vars
      let num=1
      let block+=1
      unset ifiles
    done
    # Merge the resulting files
    if [ ${#outs[@]} -eq 1 ]; then
      mv $outs $out
    else
      # ncecat -O $out ${outs[@]}
      echo "Merging output."
      t=$(date +%s)
      cdo $flags -ensmean ${outs[@]} $out
      rm ${outs[@]} 2>/dev/null
      echo " * Merge time: $(($(date +%s) - t))"
    fi
  fi

  # Next get the best-fit red noise timescale
  # TODO: Only exit if overwrite disabled
  corrs=$out
  out=$ftimescale
  if [ -r $out ] && command ncdump -h $out | grep '_sigma('; then
    # Special
    echo "Fixing names."
    ncrename -O -h -v t_sigma,t_err $out
    ncrename -O -h -v u_sigma,u_err $out
  fi
  newer $out $corrs && return 0
  stdbuf -oL -eL python3 << PYTHON
import os
import climpy
import numpy as np
import xarray as xr
$header
# Get best fit spectrum, using climpy
dims = ('plev', 'lat')
ds = xr.open_dataset("$corrs", decode_cf=False)
lag = ds['lag']
dt = lag.values[1] - lag.values[0]
units = lag.attrs['units']
out = xr.Dataset(coords={
    'lat':  ds['lat'],
    'plev': ds['plev'],
    })
longs = {
    't': 'temperature',
    'u': 'zonal wind',
    }
timer(" * Init time")
# Iterate over variables
for var in ['t','u']:
    # Calculate
    data = ds[var].values # lag is on first dimension, right?
    tau, sigma = climpy.rednoisefit(data, dt=dt, corr=True, axis=0)
    tau, sigma = tau.squeeze(), sigma.squeeze()
    # Save
    attrs = {'long_name': longs[var] + ' autocorrelation timescale estimate', 'units': units}
    out[var] = (dims, tau, attrs)
    attrs = {'long_name': 'standard error in ' + longs[var] + ' autocorrelation timescale estimate', 'units': units}
    out[var + '_err'] = (dims, sigma, attrs)
timer(" * Fit time")
# Save
if os.path.exists("$out"):
    os.remove("$out")
out.to_netcdf("$out")
timer(" * Save time")
PYTHON
  [ $? -ne 0 ] && raise "Best fit calculation failed."

  # With CDO
  # echo "Getting lag-1 autocorrelation from the time series."
  # ntime=$(cdo ntime tmp.nc 2>/dev/null)
  # timesteps=$(seq 1 $ntime | xargs | tr ' ' ',') # list of timesteps
  # timesteps1=${timesteps#*,} # trim first timestep
  # timestepsN=${timesteps%,*} # trim last one
  # t=$(date +%s)
  # cdo $flags -timcor -seltimestep,$timesteps1 tmp.nc \
  #   -seltimestep,$timestepsN tmp.nc $fautocorr
  # rm tmp.nc # remove file
}

################################################################################
# EOFs with xarray and climpy
################################################################################
climate_variability() {
  # Concatenate and select
  # WARNING: If you cancel this process while series.nc is being created,
  # EOF process may fail because file didn't finish writing!
  # if true; then #! [ -r ${pseries}.nc ]; then
  # [ -r ${peofs}_nh.nc ] && return 0
  files=($(get_files $psummary)) # full record
  ntime=$(ncdump -h "${pseries}.nc" 2>/dev/null | grep UNLIMITED | tr -dc '[0-9]') # format should be 'time = UMLIMITED; // (0 currently)'
  [ -z "$ntime" ] && ntime="0"
  # if true; then
  if ! newer ${pseries}.nc ${files[@]} || [ "$ntime" -eq 1 ]; then
    echo "Merging days"
    cdo $flags -select,name='ke,km,u,emf,ehf,t' ${files[@]} ${pseries}.nc
  fi

  # Get EOFs for each region
  # NOTE: We no longer save SH modes, and also no longer calculate vertical averages
  export PYTHONUNBUFFERED=1
  for region in nh; do
    newer ${peofs}_${region}.${days}.nc ${pseries}.nc && continue
    echo "EOFs for region: $region" # will be 20N to 70N
    python3 $eof $region 0 ${pseries}.nc ${peofs}_${region}.${days}.nc
    [ $? -ne 0 ] && raise "EOF failed."
  done
  return 0
}

################################################################################
# Time series of global energy terms
################################################################################
# For this we use "all files", i.e. include spinup before climo_start and any
# days after climo_end.# Also gets NH/SH averages, so can get meaningful autocorrelation stats.
# WARNING: CDO is fucked up and will silently compute wrong results if you
# try to use setgridarea or setgrid before getting fldmean. Instead you *must*
# add a nonsense longitude dimension before averaging.
# WARNING: Get 'inconsistent dimension definition' warning on fldmean perhaps
# because longitude dim was created after other dims. Taking *raw fldmean*
# returns array of *all missing vals* (with monde anaconda cdo, but not with
# builtin cdo), but ***sellonlatbox*** seems to fix whatever issue was going
# on with param ids (see pe_cdo.txt and pe_nco.txt in hs_base_t42l10s).
climate_lorenz() {
  # Remove old ones
  unset regions
  for file in *energy.nc *energy_nh.nc *energy_sh.nc; do
    echo "Warning: Removing bad energy file ${file}."
    rm $file
  done
  # Figure out which files need to be retrieved
  files=($(get_files $psummary 0)) # always start at day 0
  for region in nh; do # now just nh
    out=${fenergy%%.*}_${region}.${fenergy#*.}
    out=${out/d????-/d0000-}
    newer $out ${files[@]} || regions+=("$region")
  done
  [ "${#regions[@]}" -eq 0 ] && return 0
  # Iterate over files, because 128 parallel process limit
  i=0
  pmax=1
  echo "Getting time series of energy budget."
  for i in $(seq 1 ${#files[@]}); do
    # Calculate global energy budget in parallel for restart day blocks
    # echo "File: $i"
    file=${files[i-1]}
    {
      cdo $flags -mulc,101325 -vertmean -selname,$energy_params $file energy.${i}.nc
      ncap2 -A -s 'lon[$lon] = float(0.0)' energy.${i}.nc
      ncatted -a long_name,lon,o,c,'longitude' \
              -a units,lon,o,c,'degrees_east' \
              -a axis,lon,o,c,'Y' energy.${i}.nc
      for region in $regions; do
        cdo --no_warnings $flags -fldmean $(selregion $region) \
          energy.${i}.nc energy_${region}.${i}.nc
      done
    } &
    [ $((i % pmax)) -eq 0 ] && wait
  done
  wait

  # Merge files
  echo "Merging files."
  for region in $regions; do
    out=${fenergy%%.*}_${region}.${fenergy#*.}
    out=${out/d????-/d0000-}
    newer $out energy_${region}.*.nc && continue
    {
      cdo $flags -mergetime energy_${region}.*.nc $out # merge the means
      ncks --no-abc -O -C -x -v lon,lat $out $out
    } &
  done; wait
  rm energy.*.nc energy_*.*.nc
  return 0
}

################################################################################
# Spindown stuff
################################################################################
# Create ensemble-mean latitude cross-section of spindown, with time axis preserved
# * Old approach had us creating massive time-merged files of each spindown run, then
#   taking the ensemble mean of each massive file
# * New approach just has us get the ensemble mean of each day-range, then merge the time
#   axis of the small group of ensemble means
# * Each iteration of loop in new approach takes about as long as iterations from old approach (a couple minutes),
#   and the final step doesn't hang anymore (not sure if it ever would have finished).
spindown_ensemble_mean_ts() {
  # Verify files present
  spindown_files_by_day
  # First determine unique groups of spindown days
  count= # start as empty
  outfiles=() # save temporary files
  echo "Getting ensemble mean from ${#spindowndayfiles[@]} groups."
  for spindowngroup in "${spindowndayfiles[@]}"; do
    # First run a simple check
    spindowngroup=($spindowngroup) # the space-separated list of files is now expanded into an array
    newcount=${#spindowngroup[@]}
    members=(${spindowngroup[@]##*/${psummary}.})
    members=(${members[@]%%-*})
    [ ! -z $count ] && [ $count != $newcount ] && \
      raise "$newcount spindown runs in this group, but $count files in previous groups."
    count=$newcount
    # Next get ensemble mean of files; timesteps will be adopted from the first input file
    daystring=${spindowngroup[0]#$input/${psummary}.d[0-9][0-9][0-9][0-9]-spindown$mode-}
    daystring=${daystring%.nc}
    outfile=$input/spindown${daystring}.nc
    echo "Spindown runs in the ${daystring} group: ${members[@]}."
    cdo $flags -ensmean ${spindowngroup[@]} $outfile &>/dev/null # log not necessary here
    outfiles+=($outfile)
  done
  wait # wait for everything
  # From results, get ensemble mean of full spindown process
  echo "Merging the ensemble means: ${outfiles[@]##*/}."
  cdo $flags -mergetime ${outfiles[@]} $fspinclimate
  rm ${outfiles[@]}
}

# Create files with 'record' dimension showing global-average and polar-average
# spindown process for every branched spindown run
# Use CDO ngrids to create temporary fix for files with globally averaged values
spindown_regional_mean_ts() {
  # Initial stuff
  counter=0 # counter for waiting
  spindown_files_by_init
  echo "Creating records of individual spindown runs."

  # Get time-averages of spindown files from each starting date
  for region in globe polenh polesh; do
    outfiles=()
    spindowndays="" # empty string
    selregion="$(selregion $region)"
    echo "Average over ${selregion} ($region)."
    for spindowngroup in "${spindowntsfiles[@]}"; do
      # Initial stuff
      counter=$(($counter+1))
      spindowngroup=($spindowngroup) # the space-separated list of files is now expanded into an array
      spindownday=${spindowngroup[0]#$input/${psummary}.}
      spindownday=${spindownday%%-*} # the spindown day
      echo "Files in ${spindownday} run: ${spindowngroup[@]##*spindown?-}."
      # Create spindown files
      # * Accomadate old files with two grids (global energy budget variables,
      #   and normal latitude-slice variables)
      # * Beware very strange issue; if combine selgrid with sellevidx/seltimestep, with the
      #   latter coming after sellonlatbox, get error 'longitude dimension is too small'; BUG
      outfile=$input/spindown${spindownday#d}${region}.nc
      commands=("${spindowngroup[@]/#/ -fldmean $selregion -selgrid,1 }") # average
      cdo $flags -mergetime ${commands[@]} $outfile &>/dev/null & # log not necessary here
      [ $(($counter % 10)) -eq 0 ] && wait # only do a handful at a time
      # [ $counter -eq 2 ] && wait # only do a handful at a time
      # [ $counter -eq 2 ] && break # testing
      outfiles+=($outfile) # add outfile
      spindowndays+="${spindownday#d}," # add spindown day
    done; wait

    # From results, create ensemble file of spindown process
    # CDO can't read 5-D files so no more CDO processing hereafter
    echo "Getting ensemble record of spindown runs from files: ${outfiles[@]##*/}."
    ensfile=${filename}_spindown${mode}${region}.nc
    ncecat -O -u member ${outfiles[@]} $ensfile
    ncks --no-abc -O -4 --fix_rec_dmn member $ensfile $ensfile
    ncks --no-abc -O -4 --mk_rec_dmn time $ensfile $ensfile
    ncap2 -O -s "member[\$member]={${spindowndays%,}}" $ensfile $ensfile
    ncatted -O -a long_name,member,o,c,"day of initiation from control run branch" \
               -a units,member,o,c,"days since 0000-00-00 00:00:00" $ensfile
    ncks --no-abc -O $ensfile $ensfile # alphabetize output
      # fixes record dimension; see https://sourceforge.net/p/nco/bugs/85/
      # the -4 is needed or an error is thrown, weirdly
    rm ${outfiles[@]}
  done

  # Special treatment where we want to average cross-sections from each hemisphere
  # Maybe modify this maybe
  file1=(${filename}_*polenh.nc) file2=(${filename}_*polesh.nc)
  [[ ${#file1[@]} != 1 || ${#file2[@]} != 1 ]] && raise "Had issues averaging poles together."
  ncea $file1 $file2 $fspindownpoles
  rm $file1 $file2 # each pole should be thought of as additional ensemble member; so 50 runs == 100
}

# Timescale stuff
dynamical_timescale() {
  echo "Esimating dynamical timescale."
  ! [ -r $fspindownpoles ] && raise "Spindown file over poles $fspindownpoles is not available."
  ! [ -r $fclimate ] && raise "The climate file $fclimate is not available."
  # Python approach due to illegible ensemble data
  # python3 -c | tee ${input}/timescale.log << EOF
  export PYTHONUNBUFFERED=1
  import=postprocess_funcs # name of module
  python3 $dyn $fspindownpoles $fclimate $fdyntimescale 
  ncks --no-abc -O -x -v lon,lat,time $fdyntimescale $fdyntimescale # drop vars and alphabetize
}

################################################################################
# Main function for applying post-processing
################################################################################
driver() {
  # Forcing data
  if $forcing; then
    if ! newer $output/forcing.nc $input/../forcing.nc; then
      echo "Copying forcing data."
      cp $input/../forcing.nc $output
    fi
  fi

  # Climate means
  if $climate_plev; then
    echo "Control climate."
    log=${input}/climate.log
    parallel control_climate $psummary $fclimate
  fi
  if $climate_isen; then
    echo "Isentropic control climate."
    log=${input}/climate_isen.log
    parallel control_climate $psummary_isen $fclimate_isen
  fi
  if $climate_spectral; then
    echo "Average phase-speed spectra."
    log=$input/climate_spectral.log
    parallel climate_spectral
  fi

  # Autocorrelation, EOFs, time series
  if $climate_autocorr; then
    echo "Autocorrelation for pointwise data."
    log=$input/climate_autocorr.log
    parallel climate_autocorr
  fi
  if $climate_variability; then
    echo "Control 2D EOFs."
    log=$input/climate_eofs.log
    # parallel climate_variability
    parallel climate_variability
  fi
  if $climate_lorenz; then
    echo "Control global energy budget time series."
    log=$input/climate_energy.log
    parallel climate_lorenz
  fi

  # Spindown processing
  # Get the full cross-section
  if $spindown_ensemble_ts; then
    echo "Spindown ensemble mean time series."
    log=$input/spindown${mode}_mean.log
    parallel spindown_ensemble_mean_ts
  fi
  # Preserve each record but take area averages
  if $spindown_regional_ts; then
    echo "Spindown regional average time series for each branch."
    log=$input/spindown${mode}_ensemble.log
    parallel spindown_regional_mean_ts
  fi
  # Timescale
  if $spindown_timescale; then
    echo "Spindown dynamical timescale."
    log=$input/spindown${mode}_timescale.log
    parallel dynamical_timescale
  fi
}

# Call driver!
# Nice to organize it this way, so the final block of code shows up in ctags
driver
exit 0

