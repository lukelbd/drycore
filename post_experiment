#!/usr/bin/env bash
usage="post_run [OPTION...] FILE_PREFIX INPUT_DIR OUTPUT_DIR"
doc="
This script post-processes the raw model output files generated from the
GFDL dry core model. Needs to be supplied with a source and destination directory.

Usage

  $usage

Required arguments

  FILE_PREFIX  The filename prefix used to search for relevant NetCDF files.
  INPUT_DIR    The input directory used to search for these files. We search
               the 'netcdf' subfolder inside this directory.
  OUTPUT_DIR   The output directory to which averages will be sent.

Optional arguments

 -o|--overwrite             Overwrite newer files.
 -c|--constants             Copy constants from experiment scratch disk to data folder
 -cp|--climate-plev         Average climate stats from pressure lev data.
 -cs|--climate-spectral     Average climate stats from 2D-spectral decomps.
 -ci|--climate-isentropic   Average climate stats from isentropic lev data.
 -cv|--climate-variability  EOF decomps from pressure lev data.
 -ca|--climate-autocorr     Temperature autocorrelation from pressure lev data.
 -te|--timeseries-energy    Lorenz energy budget time series.
 -ts|--timeseries-surface   Surface parameters w.r.t. latitude time series.
 -se|--spindown-ensemble    Ensemble mean spindown cross-section from pressure lev data.
 -sr|--spindown-regional    Spindown data from each ensemble averaged over some region.
 -st|--spindown-timescale   Spindown dynamical timescale calculated from each ensemble (?   
 -r=*|--restart-days=*      Restart days blocks.
 -c1=*|--climate-start=*    Start day for climate averages.
 -c2=*|--climate-end=*      End day for climate averages.
 -s1=*|--spindown-start=*   Start day for spindown stats, relative to spindown start.
 -s2=*|--spindown-end=*     End day for spindown stats, relative to spindown start.
 -sm=*|--spindown-mode=*    The spindown mode.
"
# NOTE: ncrcat is generally faster than mergetime but performs *no sorting*
# so make sure the files are in correct time order.
# WARNING: Destination directory should really be on a backed up disk.
# WARNING: CDO has hard-coded maximum chained processes of 64. Horrible. Use
# commands like 'select,key=value' to merge multiple files in one command instead
# of constructing massive chained commands in single bash string.
shopt -s nullglob # will return empty if no match
export PYTHONUNBUFFERED=1 # always use line buffering, not block buffering
export TIMEFORMAT=$'real %3R user %3U sys %3S'
echoerr() {
  echo "$@" 1>&2
}
raise() {
  echoerr "Usage: $usage"
  echoerr "Error: $*"
  exit 1
}


# Environmental variables, with defaults
flags='-s -O'  # overwrite, and only issue warnings
subfolder=true  # use subfolder? have this so if we just got constants and have no real data yet
overwrite=false
smode=0
rdays=100
constants=false
climate_plev=false
climate_isen=false
timeseries_energy=false
timeseries_surface=false
climate_autocorr=false
climate_variability=false
climate_spectral=false
spindown_start=0
spindown_end=100000
spindown_timescale=false
spindown_ensemble_ts=false
spindown_regional_ts=false

# Scripts
# shellcheck disable=2034
avespec=$(pwd)/post_spectral.py
python_autocorr=$(pwd)/post_autocorr.py
python_timescale=$(pwd)/post_timescale.py
f2c=$(pwd)/post_f2c.py # converts frequency-wavenumber data to phase speed-wavenumber space
eof=$(pwd)/post_eofs.py
dyn=$(pwd)/post_dyntau.py

# Region selections expanded with ${!region}
# shellcheck disable=2034
{
  nh='-sellonlatbox,0,0,20,70'
  sh='-sellonlatbox,0,0,-70,-20'
  nhpole='-sellonlatbox,0,0,60,90'
  shpole='-sellonlatbox,0,0,-90,-60'
  globe='-sellonlatbox,0,0,-90,90'
  ncglobe='lat,-90.0,90.0'
  ncnh='lat,0.0,90.0'
  ncsh='lat,-90.0,0.0'
}

# Parse user input
while [ $# -gt 0 ]; do
  case $1 in
    -h) echo "$doc"; exit ;;
    -o|--overwrite)            overwrite=true ;;
    -c|--constants)            constants=true ;;
    -cp|--climate-plev)        climate_plev=true ;;
    -cs|--climate-spectral)    climate_spectral=true ;;
    -ci|--climate-isentropic)  climate_isen=true ;;
    -cv|--climate-variability) climate_variability=true ;; # variability
    -ca|--climate-autocorr)    climate_autocorr=true ;;
    -te|--timeseries-energy)   timeseries_energy=true ;;
    -ts|--timeseries-surface)  timeseries_surface=true ;;
    -se|--spindown-ensemble)   spindown_ensemble_ts=true ;;
    -sr|--spindown-regional)   spindown_regional_ts=true ;;
    -st|--spindown-timescale)  spindown_timescale=true ;;
    -r=*|--restart-days=*)     rdays=${1##*=} ;;
    -c1=*|--climate-start=*)   climate_start=${1##*=} ;;
    -c2=*|--climate-end=*)     climate_end=${1##*=} ;;
    -s1=*|--spindown-start=*)  spindown_start=${1##*=} ;;
    -s2=*|--spindown-end=*)    spindown_end=${1##*=} ;;
    -sm=*|--spindown-mode=*)   smode=${1##*=} ;;
    -*) raise "Unknown flag '$1'.";;
    *) # 3 required arguments, set in this order
      if [ -z "$filename" ]; then
        filename="$1"
      elif [ -z "$input" ]; then
        input="$1"
      elif [ -z "$output" ]; then
        output="$1"
      else raise "Too many arguments. Got $filename $input $output"
      fi ;;
  esac
  shift
done

# Requirements
[ -n "$climate_start" ] || raise "Specify start day with c1=day."
[ -n "$climate_end" ] || raise "Specify end day with c2=day."
if [ -z "$filename" ] || [ -z "$input" ] || [ -z "$output" ]; then
  raise "Need filename prefix (e.g. 2xdaily_inst), input folder, and output folder, in that order."
fi
if $subfolder; then
  [ -d "$input/netcdf" ] || { [ -d "$input" ] && mkdir "$input/netcdf"; }
  input="$input/netcdf"
fi

# File management/names
pfull=${filename}_full
psummary=${filename}_summary # original prefix file names
pspectral=${filename}_spectral
psummary_isen=${filename}_summary_isen

# Destinations for output files
# fregion=${filename}_timeseries.${days}.nc
# fregion_isen=${filename}_timeseries_isen.${days}.nc
period=d$(printf "%05d" "${climate_start}")-d$(printf "%05d" "${climate_end}")
feofs=${filename}_eofs.${period}.nc
fspectral=${filename}_spectral.${period}.nc
fautocorr=${filename}_autocorr.${period}.nc
ftimescale=${filename}_autocorr_timescale.${period}.nc
fclimate=${filename}_climate.${period}.nc
fclimate_isen=${filename}_climate_isen.${period}.nc
fenergy=${filename}_energy.${period}.nc
fsurface=${filename}_surface.${period}.nc
fspinclimate=${filename}_spindown${smode}_climate.${period}.nc # cross-section
fspindownpoles=${filename}_spindown${smode}_timeseries.${period}.nc # spindown rate at the poles
fdyntimescale=${filename}_dynamical_timescale${smode}.${period}.nc # the timescale stuff

# Directory management; move to save directory
# cwd=$(pwd)  # where scripts stored (you must run this script from current directory!)
[ -d "$input" ] || raise "Cannot find input directory '$input'."
[ -d "$output" ] || mkdir "$output"
cd "$output" || raise "Failed to move to ${output}."

#-----------------------------------------------------------------------------#
# Helper function
#-----------------------------------------------------------------------------#
# Check if destination file is *newer* than source files
newer() {
  local src file date idate
  dest="$1"
  src=("${@:2}")
  if [ ${#src[@]} -eq 0 ]; then
    echoerr "Skipping (no source files found)."
    return 0
  fi

  # If version without day string exists, remove it
  idest="${dest%%.*}.nc"
  [ "$dest" != "$idest" ] && [ -r "$idest" ] && {
    rm "$idest"
    echoerr "Warning: Removed file '$idest' with no day string."
  }

  # Test if sources are available; never try to run processing script if not
  for file in "${src[@]}"; do
    [ -r "$file" ] || { # TODO: test with ncdump? nah.
      echoerr "Skipping (source file ${file} unreadable)."
      return 0 # do not re-process since a source is unavailable!
    }
  done

  # Check if destination exists
  [ -r "$dest" ] || {
    echoerr "Running (${dest##*/} not found)."
    return 1
  } # re-process since destination is unavailable!

  # Finally test dates, and check for curropt files
  date=$(date +%s -r "$dest" 2>/dev/null) # just get this once
  for file in "${src[@]}"; do
    idate=$(date +%s -r "$file" 2>/dev/null)
    [ "$idate" -gt "$date" ] && {
      echoerr "Running (${dest##*/} older than source file(s))."
      return 1 # destination is *not* newer, re-process!
    }
  done

  # Test if file valid
  # WARNING: Common error is length-zero time dimension
  ncdump -h "$dest" &>/dev/null || {
    echoerr "Running (${dest##*/} appears corrupt)."
    return 1
  }
  ntime=$(ncdump -h "$dest" | grep UNLIMITED | tr -dc 0-9) # format should be 'time = UMLIMITED; // (0 currently)'
  [ -n "$ntime" ] && [ "$ntime" -eq 0 ] && {
    echoerr "Running (${dest##*/} has zero-length time dim)."
    return 1
  }

  # Final
  if $overwrite; then
    echoerr "Overwriting up-to-date file."
    return 1
  else
    echoerr "Skipping (${dest##*/} exists and is new)."
    return 0
  fi
}

#-----------------------------------------------------------------------------#
# Get lists of files
#-----------------------------------------------------------------------------#
# Get start and end day from
get_days() {
  daystring=${file##*/}
  daystring=${daystring%.nc}
  daystring=${daystring#*.}
  iday1=${daystring%%-*}
  iday1=$(printf "%.f" "${iday1#d}") || raise 'Failed to get day 1.'
  iday2=${daystring##*-}
  iday2=$(printf "%.f" "${iday2#d}") || raise 'Failed to get day 2.'
  echo "$iday1" "$iday2"
}

# Climate files within day range (e.g. to ignore spinup)
# Used for climate means and climate variability stuff
# NOTE: This is generally called in *subshell* to capture its stdout so
# 'raised' error messages that call exit will not exit this script.
get_files() {
  # Parse args
  local prefix file files ffiles
  local daystring day1 day2 days iday1 iday2 maxmissing
  maxmissing=5  # max number of files missing to establish climatology
  # maxmissing=0
  day1=$climate_start
  day2=$climate_end
  while [ $# -gt 0 ]; do
    case $1 in
      -d1=*|--day1=*)         day1=${1#*=} ;;
      -d2=*|--day2=*)         day2=${1#*=} ;;
      -mm=*|--max-missing=*)  maxmissing=${1#*=} ;;
      -*)
        raise "Unknown args passed to get_files."
      ;;
      *)
        if [ -n "$prefix" ]; then
          raise "Too many prefixes passed to get_files."
        fi
        prefix="$1"
      ;;
    esac
    shift
  done
  if [ $(((day2 - day1) % rdays)) -ne 0 ]; then
    raise "Non-integer number of $rdays-day blocks between days $day1 and $day2."
  fi

  # Get files within day range
  files=($input/${prefix}.d?????-d?????.nc)
  if [ ${#files[@]} -eq 0 ]; then
    raise "No $prefix files found in ${input}."
  fi
  echoerr "Files: ${files[*]##*/}"
  for file in "${files[@]}"; do
    idays=($(get_days "$file"))
    iday1=${idays[0]}
    iday2=${idays[1]}
    if [ $((iday2 - iday1)) -ne "$rdays" ]; then
      continue
    fi
    if [ "$iday1" -ge "$day1" ] && [ "$iday2" -le "$day2" ]; then
      days+=($iday1)
      ffiles+=($file)
    fi
  done
  if [ ${#ffiles[@]} -eq 0 ]; then
    raise "No $prefix $rdays-day blocks found between days $day1 and ${day2}."
  fi

  # Permit only up to N files missing. Sometimes accidentally delete or
  # corrupt files while testing things. For spectral data, should also have
  # *half-day* files e.g. 500-600, 550-650, 600-700, etc.
  if [[ "$prefix" =~ spectral ]]; then
    nexpected=$((2 * ( (day2 - day1) / rdays) - 1))
  else
    nexpected=$(( (day2 - day1) / rdays))
  fi
  nfiltered=${#ffiles[@]}
  message="Expected $nexpected $rdays-day blocks between days $day1 and ${day2}, got ${#ffiles[@]}. Days: ${days[*]}"
  if [ ${#ffiles[@]} -ne $nexpected ]; then
    # if [ $((nexpected - nfiltered)) -le 100 ]; then
    if [ $((nexpected - nfiltered)) -le $maxmissing ]; then
      echoerr "Warning: $message"
    else
      raise "$message"
    fi
  fi
  echo "${ffiles[@]}" # intended for user to capture output
}

# Spindown files grouped by initialization (branch) day
# Used for determining if spindowns are 'unique'
# TODO: Better print message
spindown_files_by_init() {
  echo "Getting list of spindown files within days $spindown_start to $spindown_end, grouped by control run initiation day."
  day1s=()
  for file in $input/${psummary}.d?????-spindown$smode-d?????-d?????.nc; do
    day1=${file#$input/${psummary}.}
    day1=${day1%%-*} # the spindown day
    day1s+=($day1)
  done
  day1s=($(echo "${day1s[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')) # unique days
  spindowntsfiles=()
  for day1 in "${day1s[@]}"; do
    spindowngroup="" # for particular days
    for file in $input/${psummary}.${day1}-spindown$smode-d?????-d?????.nc; do
      daystring=${file#$input/${psummary}.${day1}-spindown$smode-} # trim leading pattern
      daystring=${daystring%.nc}
      iday1=${daystring%%-*} iday1=${iday1#d}
      iday2=${daystring##*-} iday2=${iday2#d}
      [ "$iday1" -ge "$spindown_start" ] && [ "$iday2" -le "$spindown_end" ] && spindowngroup+=" $file"
    done
    spindowntsfiles+=("$spindowngroup") # the group of files is preserved as single array element separated by strings
  done
  [ ${#spindowntsfiles[@]} -eq 0 ] && raise "No $psummary files found between days ${spindown_start} and ${spindown_end}."
}

# Spindown files grouped by daystring (days after control run)
# Used for dynamical timescale stuff
# TODO: Better print message
spindown_files_by_day() {
  echo "Getting list of spindown files within days $spindown_start to $spindown_end, grouped by run day."
  daystrings=()
  for file in $input/${psummary}.d?????-spindown$smode-d?????-d?????.nc; do
    daystring=${file#$input/${psummary}.d?????-spindown$smode-} # trim leading pattern
    daystring=${daystring%.nc}
    iday1=${daystring%%-*} iday1=${iday1#d}
    iday2=${daystring##*-} iday2=${iday2#d}
    [ "$iday1" -ge "$spindown_start" ] && [ "$iday2" -le "$spindown_end" ] && daystrings+=("$daystring")
  done
  daystrings=($(echo "${daystrings[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')) # unique days
  spindowndayfiles=()
  for daystring in "${daystrings[@]}"; do
    spindowngroup="" # for particular days
    for file in $input/${psummary}.d?????-spindown$smode-${daystring}.nc; do
      spindowngroup+=" $file"
    done
    spindowndayfiles+=("$spindowngroup") # the group of files is preserved as single array element separated by strings
  done
  [ ${#spindowndayfiles[@]} -eq 0 ] && raise "No $psummary files found between days ${spindown_start} and ${spindown_end}."
}

#-----------------------------------------------------------------------------#
# Individual functions for getting climate variables
#-----------------------------------------------------------------------------#
# Control climate data, and reference to other control climates
# * Cannot chain the merge command; must be highest level no matter what. So
#   prepend timmean commands to files being merged.
# * For some reason doing -merge on chained/interpolated data results in duplicate
#   pressure level coordinates; interpolating them individually avoids this, and 
#   explicitly re-declaring the z-axis description also avoids this.
climate() {
  # Calculate with CDO
  # NOTE: Important that the array expansion is *unquoted*
  prefix=$1
  out=$2
  files=($(get_files "$prefix"))
  newer "$out" "${files[@]}" && return 0
  [ -r "$out" ] && rm "$out"
  echo "Getting time-means: ${files[*]##*/}"
  # Complicated CDO command that epicly fails sometimes
  # commands=("${files[@]/#/ -timmean }")
  # cdo $flags -ensmean ${commands[@]} "$out"
  # Simple record average
  ncra -O "${files[@]}" "$out"
}

# Average spectral runs
# TODO: Add support for pre-average windowing, or post-average Gaussian smoothing
# as in Lorenz and Hartmann.
climate_spectral() {
  # Merge spectral files with xarray and Dask
  # WARNING: Python averaging causes dask to hang indefinitely. ncea is generally quick.
  out=$fspectral
  files=($(get_files "$pspectral"))
  if ! newer "$out" "${files[@]}"; then
    echo "Getting average spectra."
    [ -r "$out" ] && rm "$out"
    time ncea "${files[@]}" "$out" || raise "Merge failed."
    # python3 "$avespec" "${files[@]}" "$out" || raise "Merge failed."
  fi

  # Always delete full spectral files
  if [ "${#files[@]}" -gt 0 ]; then
    echo "Warning: Deleting spectral data to conserve space."
    rm "${files[@]}" || raise "Deletion failed."
  fi

  # Translate from k-frequency space to k-phase speed space
  rm ${out%.nc}_phase.nc 2>/dev/null  # old file version
  out_phase=${out/spectral/spectral_phase}
  newer "$out_phase" "$out" && return 0
  [ -r "$out_phase" ] && rm "$out_phase"
  t=$(date +%s)
  python3 "$f2c" "$out" "$out_phase" \
    || raise "Translation failed."
  echo " * Time for translating to phase speeds: $(($(date +%s) - t))s."
}

# Autocorrelation
# Returns autocorrelation up to N lags, from which we can perform a best-fit
# and get a "timescale"
climate_autocorr() {
  # Get the autocorrelation spectrum in blocks to improve speed
  # Then average the blocks. Sort of like how we get Fourier transform.
  local day iday idays ifile ifiles files maxmissing
  unset outs
  out=$fautocorr
  files=($(get_files "$pfull"))
  newer "$out" "${files[@]}" && return 0
  i=0
  day=$climate_start
  # nlag=100  # up to lag-N days
  # nblock=500  # process in blocks of N days
  # maxmissing=1
  nlag=30  # up to lag-N days
  nblock=100  # process in blocks of N days
  nfiles=$((nblock / rdays))
  maxmissing=0
  while [ $day -lt $climate_end ]; do
    # Select files within range $day to $day + $nblock
    # NOTE: Permit *maxmissing* file missing in each block, and only *consecutive*
    unset ifiles
    for iday in $(seq $day $rdays $((day + nblock - 1))); do
      ifile=($(get_files "$pfull" -d1=$iday -d2=$((iday + rdays)) 2>/dev/null))
      [ ${#ifile[@]} -gt 1 ] && raise 'Duplicate files detected.'
      [ ${#ifile[@]} -eq 0 ] && [ ${#ifiles[@]} -eq 0 ] && continue  # delay start of block
      [ ${#ifile[@]} -eq 0 ] && [ ${#ifiles[@]} -gt 0 ] && break  # prematurely end block
      ifiles+=("${ifile[0]}")
    done

    # Calculate autocorrelation
    echo "Using files: ${ifiles[*]##*/}"
    if [ ${#ifiles[@]} -lt $((nfiles - maxmissing)) ]; then
      echoerr "Warning: Too many files missing from autocorrelation block ${day}."
    else
      iout=${out%.nc}.${day}.nc
      python3 "$python_autocorr" "$nlag" "${ifiles[@]}" "$iout" || raise "Autocorr calculation failed."
    fi
    day=$((day + nblock))
    outs+=($iout) # record
    unset ifiles
  done

  # Merge the resulting files
  if [ ${#outs[@]} -eq 1 ]; then
    mv "${outs[0]}" "$out"
  else
    # ncecat -O $out ${outs[@]}
    echo "Merging output."
    t=$(date +%s)
    cdo $flags -ensmean "${outs[@]}" "$out"
    rm "${outs[@]}" 2>/dev/null
    echo " * Merge time: $(($(date +%s) - t))"
  fi
}

# EOFs with xarray and climpy
climate_variability() {
  # Concatenate and select
  # WARNING: If you cancel this process while series.nc is being created,
  # EOF process may fail because file didn't finish writing!
  files=($(get_files "$psummary"))  # full record
  series=${input}/${psummary}.nc  # no day string
  ntime=$(ncdump -h "$series" 2>/dev/null | grep UNLIMITED | tr -dc 0-9) # format should be 'time = UMLIMITED; // (0 currently)'
  [ -z "$ntime" ] && ntime=0  # if file does not exist
  if ! newer "$series" "${files[@]}" || [ "$ntime" -eq 1 ]; then
    echo "Merging days"
    cdo $flags -select,name=ke,km,u,emf,ehf,t "${files[@]}" "$series"
  fi

  # Get EOFs for each region
  # NOTE: We no longer save SH modes, and also no longer calculate vertical averages
  # for region in nh sh; do
  for region in nh; do
    out=${feofs%%.*}_${region}.${feofs#*.}
    newer "$out" "$series" && continue
    echo "EOFs for region: $region" # will be 20N to 70N
    python3 "$eof" "$region" 0 "$series" "${out}" || raise "EOF failed."
  done

  # Remove series
  rm "$series" 2>/dev/null
  return 0
}

# Time series of surface zonal wind
# May add other settings in the future
# This is to understand distributon of jet winds w.r.t. time
timeseries_surface() {
  files=($(get_files "$psummary" -d1=0))  # always start at day 0

  # for region in nh sh globe; do # now just nh
  pmax=8
  for region in nh; do # now just nh
    # Check if file is newer
    ncregion=nc${region}
    out=${fsurface%%.*}_${region}.${fsurface#*.}
    out=${out/d?????-/d00000-}
    newer $out ${files[@]} && continue
    # Get surface data
    # NOTE: NCO uses zero based indexing but CDO uses 1 based indexing.
    # Can switch to 1 based in NCO with -F flag.
    # NOTE: Too many processes for cdo! Have to make temporary files.
    # # commands=("${files[@]/#/ ${!region} -sellevidx,20 -selname,u}")
    # echo "Getting surface: ${commands[*]}"
    # cdo --no_warnings $flags copy ${commands[@]} "$out"
    for i in $(seq 1 ${#files[@]}); do
      idx=$(printf "%04d" "$i")
      file=${files[i-1]}
      echo "Region selection: ${file}"
      {
        # cdo --no_warnings $flags -sellevidx,20 ${!region} -selname,u \
        #   $file surface_${region}.${idx}.nc
        ncks --no-abc -O -v u -d plev,-1 -d "${!ncregion}" \
          $file surface_${region}.${idx}.nc
      } &
      [ $((i % pmax)) -eq 0 ] && echo 'Waiting...' && wait
    done
    echo 'Waiting...' && wait
    # Merge the files
    files=(surface_${region}.????.nc)
    [ ${#files[@]} -eq 0 ] && raise "Failed to get surface data."
    echo "Merging files: ${files[*]}"
    ncrcat -O "${files[@]}" "$out"
    # Remove surface files
    rm surface_${region}.*.nc 2>/dev/null
  done
}

# Time series of Lorenz energy terms
# For this we use "all files", i.e. include spinup before climate_start and any
# days after climate_end.# Also gets NH/SH averages, so can get meaningful autocorrelation stats.
# WARNING: CDO is messed up and will silently compute wrong results if you try to use
# setgridarea or setgrid before getting fldmean. Instead you *must* add a dummy
# longitude dimension before averaging.
# WARNING: Get 'inconsistent dimension definition' warning on fldmean perhaps because
# longitude dim was created after other dims. Taking *raw fldmean* returns array of
# *all missing vals* (with monde anaconda cdo, but not with builtin cdo), but
# ***sellonlatbox*** seems to fix whatever issue was going on with param ids (see
# pe_cdo.txt and pe_nco.txt in hs_base_t42l10s).
timeseries_energy() {
  # Remove old ones
  unset regions
  for file in *energy.nc *energy_nh.nc *energy_sh.nc; do
    echo "Warning: Removing bad energy file ${file}."
    rm $file
  done

  # Figure out which files need to be retrieved
  files=($(get_files "$psummary" -d1=0))
  # for region in nh sh globe; do # now just nh
  for region in nh; do # now just nh
    out=${fenergy%%.*}_${region}.${fenergy#*.}
    out=${out/d?????-/d00000-}
    newer $out ${files[@]} || regions+=("$region")
  done
  [ ${#regions[@]} -eq 0 ] && return 0

  # Iterate over files, because 128 parallel process limit
  i=0
  pmax=8
  echo "Getting time series of energy budget."
  for i in $(seq 1 ${#files[@]}); do
    # Calculate global energy budget in parallel for restart day blocks
    # echo "File: $i"
    file=${files[i-1]}
    {
      idx=$(printf "%04d" "$i")
      cdo --no_warnings $flags -mulc,101325 -vertmean \
        -selname,ckekm,cpeke,cpmkm,cpmpe,dke,dkm,gpe,gpm,ke,km,pe,pm \
        $file energy.${idx}.nc
      ncap2 -A -s 'lon[$lon] = float(0.0)' energy.${idx}.nc
      ncatted \
        -a long_name,lon,o,c,'longitude' \
        -a units,lon,o,c,'degrees_east' \
        -a axis,lon,o,c,'Y' \
        -a units,ckekm,o,c,'W/m2' \
        -a units,cpeke,o,c,'W/m2' \
        -a units,cpmkm,o,c,'W/m2' \
        -a units,cpmpe,o,c,'W/m2' \
        -a units,dke,o,c,'W/m2' \
        -a units,dkm,o,c,'W/m2' \
        -a units,gpe,o,c,'W/m2' \
        -a units,gpm,o,c,'W/m2' \
        -a units,ke,o,c,'J/m2' \
        -a units,km,o,c,'J/m2' \
        -a units,pe,o,c,'J/m2' \
        -a units,pm,o,c,'J/m2' \
        energy.${idx}.nc 
      for region in "${regions[@]}"; do
        cdo --no_warnings $flags -fldmean ${!region} \
          energy.${idx}.nc energy_${region}.${idx}.nc
      done
    } &
    [ $((i % pmax)) -eq 0 ] && { echo "Wait!"; wait; }
  done
  echo "Wait!"
  wait

  # Merge files
  echo "Merging files."
  for region in "${regions[@]}"; do
    out=${fenergy%%.*}_${region}.${fenergy#*.}
    out=${out/d?????-/d00000-}
    newer $out energy_${region}.????.nc && continue
    # cdo $flags -mergetime energy_${region}.*.nc $out # merge the means
    {
      ncrcat -O energy_${region}.????.nc $out # merge the means
      ncks --no-abc -O -C -x -v lon,lat $out $out
    } &
  done
  wait
  rm energy.*.nc energy_*.*.nc
  return 0
}

# Spindown stuff
# Create ensemble-mean latitude cross-section of spindown, with time axis preserved
# * Old approach had us creating massive time-merged files of each spindown run, then
#   taking the ensemble mean of each massive file
# * New approach just has us get the ensemble mean of each day-range, then merge the time
#   axis of the small group of ensemble means
# * Each iteration of loop in new approach takes about as long as iterations from old approach (a couple minutes),
#   and the final step doesn't hang anymore (not sure if it ever would have finished).
spindown_ensemble_mean_ts() {
  # Verify files present
  spindown_files_by_day

  # First determine unique groups of spindown days
  count= # start as empty
  outfiles=() # save temporary files
  echo "Getting ensemble mean from ${#spindowndayfiles[@]} groups."
  for spindowngroup in "${spindowndayfiles[@]}"; do
    # First run a simple check
    spindowngroup=($spindowngroup) # the space-separated list of files is now expanded into an array
    newcount=${#spindowngroup[@]}
    members=(${spindowngroup[@]##*/${psummary}.})
    members=(${members[@]%%-*})
    [ -n "$count" ] && [ "$count" != "$newcount" ] && \
      raise "$newcount spindown runs in this group, but $count files in previous groups."
    count=$newcount
    # Next get ensemble mean of files; timesteps will be adopted from the first input file
    daystring=${spindowngroup[0]#$input/${psummary}.d?????-spindown$smode-}
    daystring=${daystring%.nc}
    outfile=$input/spindown${daystring}.nc
    echo "Spindown runs in the ${daystring} group: ${members[*]}."
    cdo $flags -ensmean ${spindowngroup[@]} $outfile &>/dev/null # log not necessary here
    outfiles+=($outfile)
  done
  wait # wait for everything

  # From results, get ensemble mean of full spindown process
  echo "Merging the ensemble means: ${outfiles[*]##*/}."
  cdo $flags -mergetime ${outfiles[@]} $fspinclimate
  rm ${outfiles[@]}
}

# Create files with 'record' dimension showing global-average and polar-average
# spindown process for every branched spindown run
# Use CDO ngrids to create temporary fix for files with globally averaged values
spindown_regional_mean_ts() {
  # Initial stuff
  counter=0 # counter for waiting
  spindown_files_by_init
  echo "Creating records of individual spindown runs."

  # Get time-averages of spindown files from each starting date
  for region in globe polenh polesh; do
    outfiles=()
    spindowndays="" # empty string
    echo "Average over ${!region} ($region)."
    for spindowngroup in "${spindowntsfiles[@]}"; do
      # Initial stuff
      counter=$((counter + 1))
      spindowngroup=($spindowngroup) # the space-separated list of files is now expanded into an array
      spindownday=${spindowngroup[0]#$input/${psummary}.}
      spindownday=${spindownday%%-*} # the spindown day
      echo "Files in ${spindownday} run: ${spindowngroup[*]##*spindown?-}."

      # Create spindown files
      # * Accomodate old files with two grids (global energy budget variables,
      #   and normal latitude-slice variables)
      # * Beware very strange issue; if combine selgrid with sellevidx/seltimestep, with the
      #   latter coming after sellonlatbox, get error 'longitude dimension is too small'; BUG
      outfile=$input/spindown${spindownday#d}${region}.nc
      commands=("${spindowngroup[@]/#/ -fldmean ${!region} -selgrid,1 }") # average
      cdo $flags -mergetime ${commands[@]} $outfile &>/dev/null & # log not necessary here
      [ $((counter % 10)) -eq 0 ] && wait # only do a handful at a time
      # [ $counter -eq 2 ] && wait # only do a handful at a time
      # [ $counter -eq 2 ] && break # testing
      outfiles+=($outfile) # add outfile
      spindowndays+="${spindownday#d}," # add spindown day
    done
    wait

    # From results, create ensemble file of spindown process
    # CDO can't read 5-D files so no more CDO processing hereafter
    echo "Getting ensemble record of spindown runs from files: ${outfiles[*]##*/}."
    ensfile=${filename}_spindown${smode}${region}.nc
    ncecat -O -u member ${outfiles[@]} $ensfile \
      || raise "Failed to get '$ensfile' average."
    ncks --no-abc -O -4 --fix_rec_dmn member $ensfile $ensfile
    ncks --no-abc -O -4 --mk_rec_dmn time $ensfile $ensfile
    ncap2 -O -s "member[\$member]={${spindowndays%,}}" $ensfile $ensfile
    ncatted -O \
      -a long_name,member,o,c,'day of initiation from control run branch' \
      -a units,member,o,c,'days since 0000-00-00 00:00:00' $ensfile
    ncks --no-abc -O $ensfile $ensfile # alphabetize output
    # fixes record dimension; see https://sourceforge.net/p/nco/bugs/85/
    # the -4 is needed or an error is thrown, weirdly
    rm ${outfiles[@]}
  done

  # Special treatment where we want to average cross-sections from each hemisphere
  # Maybe modify this maybe
  poles=(ncea ${filename}_spindown${smode}polenh.nc ${filename}_spindown${smode}polesh.nc)
  ncea ${poles[@]} $fspindownpoles \
    || raise "Failed to average poles together."
  rm ${poles[@]}  # each pole should be thought of as additional ensemble member; so 50 runs == 100
}

# Timescale stuff
spindown_dynamical_timescale() {
  echo "Esimating dynamical timescale."
  [ -r $fspindownpoles ] || raise "Spindown file over poles $fspindownpoles is not available."
  [ -r $fclimate ] || raise "The climate file $fclimate is not available."

  # Python approach due to illegible ensemble data
  # python3 -c | tee ${input}/timescale.log << EOF
  # import=postprocess_funcs # name of module
  python3 $dyn $fspindownpoles $fclimate $fdyntimescale 
  ncks --no-abc -O -x -v lon,lat,time $fdyntimescale $fdyntimescale # drop vars and alphabetize
}

#-----------------------------------------------------------------------------#
# Main function for applying post-processing
#-----------------------------------------------------------------------------#
run() {
  t=$(date +%s)
  [ -d "$output/logs" ] || mkdir "$output/logs" || raise "Could not make log directory"
  echo "Running: $*"
  echo "Log file: $output/logs/$log"
  "$@" &> "$output/logs/$log" || raise "Command '$1' failed."
  echo "Elapsed time: $(($(date +%s) - t))s."
}

driver() {
  # Forcing data
  if $constants; then
    # if true; then
    if ! newer $output/constants.nc $input/../constants.nc; then
      echo "Copying constants."
      cp $input/../constants.nc $output/constants.nc
    fi
  fi

  # Climate means
  if $climate_plev; then
    echo "Control climate."
    log=climate_plev.log
    run climate "$psummary" "$fclimate"
  fi
  if $climate_isen; then
    echo "Isentropic control climate."
    log=climate_isen.log
    run climate "$psummary_isen" "$fclimate_isen"
  fi
  if $climate_spectral; then
    echo "Average phase-speed spectra."
    log=climate_spectral.log
    run climate_spectral
  fi

  # Autocorrelation, EOFs, time series
  if $climate_autocorr; then
    echo "Autocorrelation for pointwise data."
    log=climate_autocorr.log
    run climate_autocorr
  fi
  if $climate_variability; then
    echo "Control 2D EOFs."
    log=climate_variability.log
    run climate_variability
  fi
  if $timeseries_energy; then
    echo "Control global energy budget time series."
    log=timeseries_energy.log
    run timeseries_energy
  fi
  if $timeseries_surface; then
    echo "Control surface parameter time series."
    log=timeseries_surface.log
    run timeseries_surface
  fi

  # Spindown processing
  # Get the full cross-section
  if $spindown_ensemble_ts; then
    echo "Spindown ensemble mean time series."
    log=spindown${smode}_mean.log
    run spindown_ensemble_mean_ts
  fi

  # Preserve each record but take area averages
  if $spindown_regional_ts; then
    echo "Spindown regional average time series for each branch."
    log=spindown${smode}_ensemble.log
    run spindown_regional_mean_ts
  fi

  # Timescale
  if $spindown_timescale; then
    echo "Spindown dynamical timescale."
    log=spindown${smode}_timescale.log
    run spindown_dynamical_timescale
  fi
}

# Call driver!
# Nice to organize it this way, so the final block of code shows up in ctags
driver
exit 0

