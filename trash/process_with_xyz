#!/bin/bash
################################################################################
# File for processing results of GFDL model runs AS THEY ARE PRODUCED; use this
# in parallel after every model step while next model step is simultaneously running.
# ---------
# Summary of current variables:
# * with longitude-data:
#   p, t, u, v, z, vor, omega, teq, D, pt, pv s
#   forget dthdp (only will want lapse rate to diagnose mean climate/tropopause
#   heights and can use the mean t/theta fields), but keep S; has physical interpretation
# * without longitude-data:
#   t, u, v, z, vor, omega, teq, D, dthdp, pt, pv, s, **NEW** EHF, EMF, EPF, EKE, C, KE
# Tips:
# Run 'grep -r -l "^[^\!].*register_diag_field(" . | grep ".*\.f90"' at base of source
# directory to see list of files where diag_fields are registered.
################################################################################
# Function for checking NCL scripts success
# Script sometimes fails to spawn child process for displaying time information; ignore these errors
# Also Execute.c is always shown as error alongside specific function error, so ignore that one
nclcheck() { # input log file as argument
  cat $1 | grep -v "Execute.c" | grep -v "systemfunc" | egrep "^fatal:" &>/dev/null
  return $? # return exit code; good status means we found error, bad status means we found nothing
    # ignore ^warning: because these will also happen if we have a systemfunc error possibly
}

# Make sure modules loaded
module load impi &>/dev/null # to parallelize the combine process, need mpirun!
module load nco &>/dev/null
module load ncl &>/dev/null
module load cdo/1.9.4 &>/dev/null

# Parse input flags
# cores=8 # default cores for mppnccombine, actually this didn't work so nevermind
debug=false     # keep data in original folder, instead of moving to ../netcdf? useful for testing
keepxyz=false   # keep original "full" results?
resume=false    # process old "full" files?
cdointerp=false # slower than NCO
combine=false   # just run mppnccombine
while [ $# -gt 0 ]; do
  case $1 in
    -q|--quit) echo "Skipping processing." && exit 0 ;;
    -d|--debug)      debug=true      ;;  # if true, keep all intermediate files
    -r|--resume)     resume=true      ;; # process old results sitting in the directory
    -k|--keep-xyz)   keepxyz=true     ;; # whether to keep longitudinal data
    -i|--interp-cdo) cdointerp=false  ;; # interpolate with CDO instead of NCL? much slower during testing (see notes)
    -c|--combine)    combine=true ;;     # only combine data
    # -np=*)           cores="${1#*=}" ;;  # number of cores
    *) echo "Error: Unknown flag \"$1\"." && exit 127 ;;
  esac
  shift # shift by one
done

# Global vars
t0=$(date +%s)                    # starting time
bin=${0%/*}                       # same location as this file; trim filename from right
run=mpirun                        # for running mppnccombine on Cheyenne; parallelization really necessary
interpcdo=$bin/process_interp     # interpolate with CDO
interpncl=$bin/process_interp.ncl # interpolate with NCL, automatic iteration
mppnccombine=$bin/mppnccombine.x  # combine; needs to have been compiled here
fluxes=$bin/process_fluxes        # zonal means and flux terms with CDO
lorenz=$bin/process_energy.ncl    # eddy and mean Lorenz energy cycle terms with NCL
# lorenz=$bin/process_lorenz      # Lorenz energy cycle terms with CDO
# energy=$bin/process_energy      # Lorenz energy cycle terms with CDO

# Double check some stuff
[ ! -x $fluxes ]        && echo "Error: The script \"$fluxes\" is missing."                       && exit 127
[ ! -x $lorenz ]        && echo "Error: The script \"$lorenz\" is missing."                       && exit 127
[ ! -r $interpncl ]     && echo "Error: The script \"$interpncl\" is missing."                    && exit 127
[ ! -x $mppnccombine ]  && echo "Error: The mpp combine executable \"$mppnccombine\" is missing." && exit 127
! hash ncl 2>/dev/null  && echo "Error: NCL is not in $PATH."                                     && exit 127
! hash cdo 2>/dev/null  && echo "Error: CDO is not in $PATH."                                     && exit 127
! hash ncks 2>/dev/null && echo "Error: NCO is not in $PATH."                                     && exit 127

################################################################################
# Functions
# Stdout of following lines is overview; view separate logfiles for specifics
################################################################################
# Function for interpolating model levels
hybrid_to_press() {
  # Interplolate to pressure levels from model levels
  local t ncfile output suffix
  ncfile="$1"
  suffix="$2"
  output=${ncfile%%.*}_interp.$suffix

  # Apply some simple modifications to attributes
  ncrename -d pfull,mlev $ncfile &>/dev/null # mlev is the ECHAM convention, needed for CDO interpolation
  ncrename -d phalf,ilev $ncfile &>/dev/null # ilev is the other ECHAM convention
  ncrename -v pfull,mlev $ncfile &>/dev/null # -d renames dimensions, -v renames variables of same name
  ncrename -v phalf,ilev $ncfile &>/dev/null
  ncatted -O -a bounds,mlev,o,c,"ilev" $ncfile

  # Interpolation
  # a) Use CDO for interpolation
  t=$(date +%s)
  if $cdointerp; then
    echo "Interpolating with CDO..."
    $interpcdo $ncfile $output &>process-logs/log.interp.${suffix%.nc} # that simple babe
    [ $? != 0 ] && echo "Error: Something failed during CDO interpolation." && exit $?
    ! $debug && rm interp?.$suffix
  # b) Use NCL vint2hp_ecmwf for interpolation
  else
    echo "Interpolating with NCL..." # -Q = no banner, -n = do not enumerate print statements
    ncl -n -Q "filename=\"$ncfile\"" "outfile=\"$output\"" $interpncl &>process-logs/log.interp.${suffix%.nc} #2> /dev/null
    if nclcheck process-logs/log.interp.${suffix%.nc} || [ ! -r $output ]; then
      echo "Error: Something failed during NCL interpolation." && exit 2
    fi
  fi

  # Standardize attributes
  # Many of these may already be properly set, but make sure; CDO also may want
  # a few special ones like 'axis' and 'standard_name'
  # NOTE: This is really fast (0.02s on T42 20 day file)
  # This is much faster than even editing one attribute in NCL (0.05s)
  ncatted -O -a NumFilesInSet,global,o,l,$numfiles \
    -a edges,lon,d,, -a edges,lat,d,, \
    -a axis,lon,o,c,"X" -a axis,lat,o,c,"Y" -a axis,plev,o,c,"Z" -a axis,time,o,c,"T" \
    -a long_name,plev,o,c,"pressure_level" -a units,plev,o,c,"mb" -a bounds,plev,o,c,"plev_bnds" \
    -a long_name,lat,o,c,"latitude"  -a standard_name,lat,o,c,"latitude" \
    -a long_name,lon,o,c,"longitude" -a standard_name,lon,o,c,"longitude" \
    -a standard_name,time,o,c,"time" \
    -a units,lat,o,c,"degrees_north" -a units,lon,o,c,"degrees_east" $output
  ! $debug && mv $output $ncfile # move old file on top of new file
  echo "  * Time for interpolation: $(($(date +%s) - t))s."
}

# Once, this function also accepted averaged data; not anymore, but these are
# some approaches for testing if file has averaged data
# * [[ " $(cdo -s showname $ncfile) " =~ " time_bounds " ]] # CDO test if averaged
#   isaveraged="print(isfilevar(addfile(\"$ncfile\",\"r\"),\"time_bounds\"))"
# * [ $(ncl -Q -n <<< "$isaveraged") == "False" ] # NCL test if averaged
# * [[ ! " $(ncvarlist $ncfile) " =~ " time_bounds " ]] # NCO test if averaged
yz_params() {
  # Ouput file names
  local t t2 ncfile output suffix
  ncfile="$1"
  suffix="$2"
  output="${yzfile%.nc}.${suffix}"

  # Fluxes and means
  t=$(date +%s)
  echo "Getting CDO YZ parameters..."
  $fluxes $ncfile $suffix &>process-logs/log.basic.${suffix%.nc} #&
  [ $? != 0 ] && echo "Error: Something failed while getting CDO parameters." && exit 3
  t2=$(($(date +%s) - t))
  # t2=$(tail -1 process-logs/log.basic.${suffix%.nc} | sed 's/[^0-9]*//g')
  echo "  * Time for getting YZ parameters with CDO: ${t2}s."

  # Energy budgets, NCL commands
  # Some of these require the extra variables output by NCL; also meridional flux
  # terms rely on NCL having set poleward==positive in each hemisphere
  t=$(date +%s)
  echo "Getting NCL YZ parameters..."
  ncl -n -Q "suffix=\"$suffix\"" "filename=\"$ncfile\"" $lorenz &>process-logs/log.lorenz.${suffix%.nc} #&
  if nclcheck process-logs/log.lorenz.${suffix%.nc}; then
    echo "Error: Something failed while getting NCL parameters." && exit 4
  fi
  t2=$(($(date +%s) - t))
  # t2=$(tail -1 process-logs/log.lorenz.${suffix%.nc} | sed 's/[^0-9]*//g')
  echo "  * Time for getting YZ parameters with NCL: ${t2}s."

  # Add back longitudes to NCL calculated terms
  # Hard/annoying/verbose to fix this in NCL so do it here
  t=$(date +%s)
  echo "Merging the CDO and NCL parameters into one file..."
  for f in energy??.$suffix; do # absolutely necessary for CDO to detect plev as vertical axis, and need singleton longitude
    cdo -s -setgrid,basic1.$suffix $f tmp.$suffix; mv tmp.$suffix $f # so ugly... kill me...
  done
  echo "  * Time for setting grid: $(($(date +%s) - t))s."

  # Merge files
  t=$(date +%s)
  cdo -O -s -merge basic?.$suffix energy??.$suffix $output # will raise errors because edges were deleted, but still referenced by attributes
  # ncks -O $output $output # clever hack; just *alphabetizes* the variables in file, and very fast
  echo "  * Time for merging into one file: $(($(date +%s) - t))s."
  ! $debug && rm basic?.$suffix energy??.$suffix

  # Simple NCL script to fix attributes
  # Add NumFilesInSet attribute (required)
  t=$(date +%s)
  ncatted -O -a NumFilesInSet,global,o,l,$numfiles $output
  ncl -n -Q <<< "
  f = addfile(\"$ncfile\", \"r\")
  o = addfile(\"$output\", \"w\")
  ; Fix attributes and whatnot
  o->lat = f->lat
  o->plev = f->plev
  o->time = f->time
  ; Fix longitudes
  lon = o->lon
  lon@domain_decomposition = (/1, 1, 1, 1/)
  o->lon = lon
  ; 'Save' (i.e. delete handle)
  delete(o)
  "
  echo "  * Time for fixing attributes: $(($(date +%s) - t))s"
}

# Gets extra XYZ parameters
xyz_params() {
  # Input
  local t t1 output
  output=$1 # then ncfiles are remaining input

  # Merge files
  t=$(date +%s)
  t1=$t # save
  $mppnccombine -r $output ${@:2}
  echo "  * Time for combining XYZ files: $(($(date +%s) - t))s"

  # Spectral divergence, vorticity
  t=$(date +%s)
  cdo -O -s -uv2dv -selname,u,v $output dv0.nc # gets spectral coefs for vor/div from Gaussian grid of u/v
  echo "  * Time for getting spectral div, vor: $(($(date +%s) - t))s"

  # Streamfunc
  t=$(date +%s)
  cdo -O -s -delname,velopot -invertlat -sp2gp -dv2ps dv0.nc dv1.nc # spectral vor/div to spectral streamfunc/velocity pot; throw out the latter
  echo "  * Time for getting streamfunc: $(($(date +%s) - t))s"

  # Gridpoint divergence, vorticity
  t=$(date +%s)
  cdo -O -s -chname,sd,div,svo,vor -invertlat -sp2gp dv0.nc dv2.nc  # better names
  echo "  * Time for getting gridpoint div, vor: $(($(date +%s) - t))s"

  # Absolute vorticity; asterisk means this is a "RAM" variable, not saved
  # NOTE: Turns out ncap2 really sucks -- absvor took 7s on 20 day T42 file
  # compared to 4s and 3s for the -sp2gp commands! Thought it was clever but
  # evidently not worth it.
  # t=$(date +%s)
  # ncap2 -O -s '*f[$lat] = 2*7.292e-5*sin(lat*3.14159/180); absvor = vor+f' dv2.nc dv2.nc # tack on absolute vorticity
  # echo "  * Time for getting absvor: $(($(date +%s) - t))s"

  # Merge or delete?
  if [ ! -r dv0.nc ] || [ ! -r dv1.nc ] || [ ! -r dv2.nc ]; then
    # Delete
    echo "Warning: Could not get the vir/div/streamfunc terms."
    mv $output $xyzfile
  else
    # Merge with CDO; due to copying, is slow (15s for 20 day T42)
    # t=$(date +%s)
    # cdo -O -s -merge $output dv1.nc dv2.nc $xyzfile
    # ! $debug && rm $output
    # echo "  * Time for merging XYZ files with CDO: $(($(date +%s) - t))s"
    # Merge with NCO, if faster (6s for 20 day T42)
    # NOTE: Weird issue where the -gp2sp then -sp2gp results in double precision
    # lat and lon coordinates, which causes ncks to think they're different.
    # NOTE: NCL cannot do this; idea was to use f->lat := tofloat(f->lat),
    # but get Translate.c internal error when attempting to change type;
    # evidently this is impossible for file variables
    t=$(date +%s)
    ncap2 -O -s 'lat=float(lat); lon=float(lon)' dv1.nc dv1.nc
    ncap2 -O -s 'lat=float(lat); lon=float(lon)' dv2.nc dv2.nc
    ncks -A dv1.nc $output
    ncks -A dv2.nc $output
    mv $output $xyzfile
    echo "  * Time for merging XYZ files with NCO: $(($(date +%s) - t))s"
  fi
  echo "TOTAL TIME ELAPSED: $(($(date +%s) - t1))s."
}

################################################################################
# Driver, calls above functions
################################################################################
# Get files
if $resume && $debug; then
  files=(*_full.nc)
elif $resume; then
  files=(../netcdf/*_full.${PWD##*/}.nc)
else
  files=(*.nc.0000)
fi
if [[ "$files" =~ "*" ]]; then
  files=(*.nc) # already present?
  if [ ${#files[@]} -ge 1 ]; then
    echo "Note: Model ran on only 1 core, do not need to merge files."
  else
    echo "Error: XYZ resolution files not found." && exit 1 # make sure nullglob turned off
  fi
fi

# Loop through each one
for file in ${files[@]}; do
  # Figure out file names and such
  if [[ "$file" =~ 0000 ]]; then
    combine=true
    base=${file%%.*}
    for part in ${base}.nc*; do
      num=${part##*.}
      mv $part ${base}.${num}.nc # some programs won't recognize file without .nc extension
    done
    parts=(${base}.*.nc)
  else
    combine=false
    parts=(${file})
  fi
  echo Files: ${parts[@]}
  numfiles=${#parts[@]}
  xyzfile="${base}_full.${PWD##*/}.nc"   # original data
  yzfile="${base}_summary.${PWD##*/}.nc" # longitude-averaged data

  # Interpolate to pressure then get YZ files from each model-produced
  # netcdf file
  echo "Getting YZ parameters..."
  mkdir process-logs
  # exit 1
  t1=$(date +%s)
  for ncfile in ${parts[@]}; do
    # Number
    if $combine; then
      num=${ncfile%.nc}
      num=${num#*.}
      suffix=${num}.nc
    else
      num=0000
      suffix=nc # dummy
    fi
    # Interpolate and process
    {
    if ! $resume; then
      hybrid_to_press "$ncfile" "$suffix"
    fi
    $debug && ncfile=${ncfile%%.*}_interp.$suffix
    yz_params "$ncfile" "$suffix"
    } &>process-logs/log.${num} &
  done
  wait
  $debug && parts=(${base}_interp.*.nc)
  echo "  * Time for getting YZ params in parallel: $(($(date +%s) - t1))s"
  # exit 1
  # Merge the parallel-processed YZ files
  if $combine; then
    t=$(date +%s)
    $mppnccombine -r $yzfile ${yzfile%.nc}.*.nc
    echo "  * Time for combining YZ files: $(($(date +%s) - t))s."
  fi

  # Optionally merge the pressure-interpolated files, then get XYZ
  # parameters, or just delete those files
  # TODO: Consider reducing to a standard horizontal resolution at least, if
  # not vertical? This could save considerable space.
  if ! $resume; then
    if ! $debug && ! $keepxyz; then
      # Delete the old file
      echo "Removing XYZ files..."
      ! $debug && rm ${parts[@]} # simple
    else
      # XYZ files
      echo "Keeping XYZ files and calculating extra terms..."
      t=$(date +%s)
      xyz_params ${base}.nc ${parts[@]} &>log.xyz
      ! $debug && rm dv?.nc # remove any that may exist
      echo "  * Time for getting XYZ params: $(($(date +%s) - t))s."
    fi
  fi

  # Copy files to common directory
  if ! $debug; then
    ! [ -d ../netcdf ] && mkdir ../netcdf
    [ -r $yzfile ]  && mv $yzfile ../netcdf
    [ -r $xyzfile ] && mv $xyzfile ../netcdf
  fi
done

# Echo time of finish
# Can parse this to send message to the window that ran the runscript/batch runscript
# echo $(($(date +%s) - $t0)) # prints UNIX time difference
echo "TOTAL TIME ELAPSED: $(($(date +%s) - t0))s."
